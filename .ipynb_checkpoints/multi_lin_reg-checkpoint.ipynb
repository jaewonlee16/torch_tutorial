{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0/10000 hypothesis: tensor([0., 0., 0., 0., 0.])  cost: 29661.800781\n",
      "epoch 100/10000 hypothesis: tensor([154.0433, 185.0925, 175.8312, 198.5701, 141.2221])  cost: 5.754568\n",
      "epoch 200/10000 hypothesis: tensor([154.0278, 185.0649, 175.9335, 198.5128, 141.2284])  cost: 5.512375\n",
      "epoch 300/10000 hypothesis: tensor([154.0120, 185.0385, 176.0329, 198.4569, 141.2353])  cost: 5.281663\n",
      "epoch 400/10000 hypothesis: tensor([153.9960, 185.0133, 176.1295, 198.4022, 141.2426])  cost: 5.061868\n",
      "epoch 500/10000 hypothesis: tensor([153.9797, 184.9892, 176.2233, 198.3488, 141.2504])  cost: 4.852397\n",
      "epoch 600/10000 hypothesis: tensor([153.9632, 184.9662, 176.3143, 198.2966, 141.2586])  cost: 4.652731\n",
      "epoch 700/10000 hypothesis: tensor([153.9465, 184.9442, 176.4028, 198.2456, 141.2672])  cost: 4.462276\n",
      "epoch 800/10000 hypothesis: tensor([153.9296, 184.9232, 176.4888, 198.1958, 141.2762])  cost: 4.280613\n",
      "epoch 900/10000 hypothesis: tensor([153.9126, 184.9032, 176.5724, 198.1471, 141.2855])  cost: 4.107294\n",
      "epoch1000/10000 hypothesis: tensor([153.8955, 184.8842, 176.6536, 198.0995, 141.2952])  cost: 3.941827\n",
      "epoch1100/10000 hypothesis: tensor([153.8782, 184.8660, 176.7325, 198.0530, 141.3051])  cost: 3.783911\n",
      "epoch1200/10000 hypothesis: tensor([153.8608, 184.8486, 176.8092, 198.0075, 141.3153])  cost: 3.633052\n",
      "epoch1300/10000 hypothesis: tensor([153.8434, 184.8320, 176.8838, 197.9630, 141.3257])  cost: 3.488966\n",
      "epoch1400/10000 hypothesis: tensor([153.8259, 184.8163, 176.9563, 197.9195, 141.3364])  cost: 3.351294\n",
      "epoch1500/10000 hypothesis: tensor([153.8083, 184.8013, 177.0268, 197.8770, 141.3473])  cost: 3.219737\n",
      "epoch1600/10000 hypothesis: tensor([153.7908, 184.7870, 177.0954, 197.8355, 141.3584])  cost: 3.093962\n",
      "epoch1700/10000 hypothesis: tensor([153.7732, 184.7734, 177.1620, 197.7948, 141.3697])  cost: 2.973687\n",
      "epoch1800/10000 hypothesis: tensor([153.7556, 184.7604, 177.2268, 197.7551, 141.3811])  cost: 2.858666\n",
      "epoch1900/10000 hypothesis: tensor([153.7380, 184.7481, 177.2899, 197.7162, 141.3927])  cost: 2.748632\n",
      "epoch2000/10000 hypothesis: tensor([153.7204, 184.7364, 177.3512, 197.6782, 141.4043])  cost: 2.643343\n",
      "epoch2100/10000 hypothesis: tensor([153.7028, 184.7253, 177.4108, 197.6410, 141.4161])  cost: 2.542561\n",
      "epoch2200/10000 hypothesis: tensor([153.6853, 184.7147, 177.4689, 197.6047, 141.4280])  cost: 2.446101\n",
      "epoch2300/10000 hypothesis: tensor([153.6678, 184.7047, 177.5253, 197.5691, 141.4400])  cost: 2.353728\n",
      "epoch2400/10000 hypothesis: tensor([153.6504, 184.6951, 177.5803, 197.5343, 141.4520])  cost: 2.265255\n",
      "epoch2500/10000 hypothesis: tensor([153.6331, 184.6861, 177.6338, 197.5003, 141.4641])  cost: 2.180530\n",
      "epoch2600/10000 hypothesis: tensor([153.6158, 184.6775, 177.6858, 197.4670, 141.4763])  cost: 2.099343\n",
      "epoch2700/10000 hypothesis: tensor([153.5987, 184.6694, 177.7365, 197.4344, 141.4884])  cost: 2.021554\n",
      "epoch2800/10000 hypothesis: tensor([153.5816, 184.6617, 177.7858, 197.4025, 141.5006])  cost: 1.946990\n",
      "epoch2900/10000 hypothesis: tensor([153.5646, 184.6544, 177.8338, 197.3714, 141.5128])  cost: 1.875518\n",
      "epoch3000/10000 hypothesis: tensor([153.5477, 184.6475, 177.8806, 197.3408, 141.5251])  cost: 1.806987\n",
      "epoch3100/10000 hypothesis: tensor([153.5309, 184.6410, 177.9261, 197.3109, 141.5373])  cost: 1.741272\n",
      "epoch3200/10000 hypothesis: tensor([153.5142, 184.6349, 177.9704, 197.2817, 141.5495])  cost: 1.678254\n",
      "epoch3300/10000 hypothesis: tensor([153.4977, 184.6290, 178.0136, 197.2531, 141.5617])  cost: 1.617801\n",
      "epoch3400/10000 hypothesis: tensor([153.4812, 184.6236, 178.0556, 197.2251, 141.5738])  cost: 1.559808\n",
      "epoch3500/10000 hypothesis: tensor([153.4650, 184.6184, 178.0966, 197.1977, 141.5860])  cost: 1.504152\n",
      "epoch3600/10000 hypothesis: tensor([153.4488, 184.6136, 178.1364, 197.1709, 141.5981])  cost: 1.450739\n",
      "epoch3700/10000 hypothesis: tensor([153.4327, 184.6090, 178.1753, 197.1446, 141.6101])  cost: 1.399473\n",
      "epoch3800/10000 hypothesis: tensor([153.4169, 184.6047, 178.2132, 197.1189, 141.6221])  cost: 1.350247\n",
      "epoch3900/10000 hypothesis: tensor([153.4011, 184.6007, 178.2501, 197.0938, 141.6340])  cost: 1.303012\n",
      "epoch4000/10000 hypothesis: tensor([153.3855, 184.5969, 178.2860, 197.0691, 141.6459])  cost: 1.257636\n",
      "epoch4100/10000 hypothesis: tensor([153.3701, 184.5934, 178.3211, 197.0450, 141.6577])  cost: 1.214077\n",
      "epoch4200/10000 hypothesis: tensor([153.3548, 184.5900, 178.3553, 197.0214, 141.6695])  cost: 1.172224\n",
      "epoch4300/10000 hypothesis: tensor([153.3396, 184.5870, 178.3885, 196.9983, 141.6811])  cost: 1.132027\n",
      "epoch4400/10000 hypothesis: tensor([153.3246, 184.5841, 178.4210, 196.9756, 141.6927])  cost: 1.093410\n",
      "epoch4500/10000 hypothesis: tensor([153.3098, 184.5814, 178.4526, 196.9535, 141.7042])  cost: 1.056317\n",
      "epoch4600/10000 hypothesis: tensor([153.2951, 184.5789, 178.4835, 196.9318, 141.7157])  cost: 1.020669\n",
      "epoch4700/10000 hypothesis: tensor([153.2805, 184.5766, 178.5136, 196.9105, 141.7270])  cost: 0.986409\n",
      "epoch4800/10000 hypothesis: tensor([153.2662, 184.5745, 178.5429, 196.8897, 141.7382])  cost: 0.953489\n",
      "epoch4900/10000 hypothesis: tensor([153.2520, 184.5726, 178.5715, 196.8693, 141.7494])  cost: 0.921829\n",
      "epoch5000/10000 hypothesis: tensor([153.2379, 184.5707, 178.5994, 196.8493, 141.7605])  cost: 0.891409\n",
      "epoch5100/10000 hypothesis: tensor([153.2240, 184.5691, 178.6266, 196.8297, 141.7714])  cost: 0.862158\n",
      "epoch5200/10000 hypothesis: tensor([153.2103, 184.5676, 178.6532, 196.8106, 141.7822])  cost: 0.834052\n",
      "epoch5300/10000 hypothesis: tensor([153.1968, 184.5662, 178.6791, 196.7918, 141.7930])  cost: 0.807019\n",
      "epoch5400/10000 hypothesis: tensor([153.1834, 184.5650, 178.7044, 196.7734, 141.8036])  cost: 0.781017\n",
      "epoch5500/10000 hypothesis: tensor([153.1702, 184.5638, 178.7290, 196.7554, 141.8142])  cost: 0.756017\n",
      "epoch5600/10000 hypothesis: tensor([153.1571, 184.5629, 178.7531, 196.7378, 141.8246])  cost: 0.731975\n",
      "epoch5700/10000 hypothesis: tensor([153.1442, 184.5620, 178.7766, 196.7205, 141.8349])  cost: 0.708852\n",
      "epoch5800/10000 hypothesis: tensor([153.1315, 184.5612, 178.7995, 196.7035, 141.8452])  cost: 0.686602\n",
      "epoch5900/10000 hypothesis: tensor([153.1189, 184.5605, 178.8219, 196.6870, 141.8553])  cost: 0.665200\n",
      "epoch6000/10000 hypothesis: tensor([153.1065, 184.5599, 178.8437, 196.6707, 141.8653])  cost: 0.644615\n",
      "epoch6100/10000 hypothesis: tensor([153.0943, 184.5594, 178.8650, 196.6548, 141.8752])  cost: 0.624813\n",
      "epoch6200/10000 hypothesis: tensor([153.0822, 184.5590, 178.8858, 196.6392, 141.8849])  cost: 0.605754\n",
      "epoch6300/10000 hypothesis: tensor([153.0703, 184.5587, 178.9061, 196.6239, 141.8946])  cost: 0.587416\n",
      "epoch6400/10000 hypothesis: tensor([153.0585, 184.5584, 178.9260, 196.6089, 141.9041])  cost: 0.569771\n",
      "epoch6500/10000 hypothesis: tensor([153.0469, 184.5583, 178.9453, 196.5942, 141.9136])  cost: 0.552791\n",
      "epoch6600/10000 hypothesis: tensor([153.0355, 184.5582, 178.9643, 196.5798, 141.9229])  cost: 0.536447\n",
      "epoch6700/10000 hypothesis: tensor([153.0242, 184.5581, 178.9827, 196.5657, 141.9321])  cost: 0.520723\n",
      "epoch6800/10000 hypothesis: tensor([153.0131, 184.5582, 179.0008, 196.5519, 141.9412])  cost: 0.505579\n",
      "epoch6900/10000 hypothesis: tensor([153.0021, 184.5583, 179.0184, 196.5383, 141.9501])  cost: 0.491019\n",
      "epoch7000/10000 hypothesis: tensor([152.9913, 184.5584, 179.0357, 196.5250, 141.9590])  cost: 0.476987\n",
      "epoch7100/10000 hypothesis: tensor([152.9806, 184.5586, 179.0525, 196.5120, 141.9677])  cost: 0.463494\n",
      "epoch7200/10000 hypothesis: tensor([152.9701, 184.5589, 179.0689, 196.4993, 141.9764])  cost: 0.450498\n",
      "epoch7300/10000 hypothesis: tensor([152.9597, 184.5591, 179.0850, 196.4868, 141.9849])  cost: 0.437985\n",
      "epoch7400/10000 hypothesis: tensor([152.9495, 184.5595, 179.1006, 196.4745, 141.9933])  cost: 0.425949\n",
      "epoch7500/10000 hypothesis: tensor([152.9395, 184.5599, 179.1160, 196.4625, 142.0016])  cost: 0.414340\n",
      "epoch7600/10000 hypothesis: tensor([152.9296, 184.5603, 179.1310, 196.4507, 142.0098])  cost: 0.403180\n",
      "epoch7700/10000 hypothesis: tensor([152.9198, 184.5607, 179.1456, 196.4392, 142.0179])  cost: 0.392427\n",
      "epoch7800/10000 hypothesis: tensor([152.9102, 184.5612, 179.1599, 196.4279, 142.0258])  cost: 0.382068\n",
      "epoch7900/10000 hypothesis: tensor([152.9007, 184.5617, 179.1739, 196.4168, 142.0337])  cost: 0.372110\n",
      "epoch8000/10000 hypothesis: tensor([152.8914, 184.5623, 179.1876, 196.4059, 142.0414])  cost: 0.362515\n",
      "epoch8100/10000 hypothesis: tensor([152.8821, 184.5629, 179.2010, 196.3952, 142.0491])  cost: 0.353266\n",
      "epoch8200/10000 hypothesis: tensor([152.8731, 184.5635, 179.2140, 196.3848, 142.0566])  cost: 0.344371\n",
      "epoch8300/10000 hypothesis: tensor([152.8642, 184.5641, 179.2268, 196.3745, 142.0640])  cost: 0.335797\n",
      "epoch8400/10000 hypothesis: tensor([152.8554, 184.5647, 179.2393, 196.3645, 142.0713])  cost: 0.327543\n",
      "epoch8500/10000 hypothesis: tensor([152.8467, 184.5654, 179.2515, 196.3546, 142.0785])  cost: 0.319596\n",
      "epoch8600/10000 hypothesis: tensor([152.8382, 184.5661, 179.2635, 196.3450, 142.0856])  cost: 0.311935\n",
      "epoch8700/10000 hypothesis: tensor([152.8298, 184.5668, 179.2752, 196.3355, 142.0926])  cost: 0.304557\n",
      "epoch8800/10000 hypothesis: tensor([152.8216, 184.5675, 179.2866, 196.3262, 142.0995])  cost: 0.297460\n",
      "epoch8900/10000 hypothesis: tensor([152.8134, 184.5683, 179.2978, 196.3171, 142.1063])  cost: 0.290619\n",
      "epoch9000/10000 hypothesis: tensor([152.8055, 184.5690, 179.3087, 196.3082, 142.1130])  cost: 0.284026\n",
      "epoch9100/10000 hypothesis: tensor([152.7976, 184.5698, 179.3194, 196.2994, 142.1196])  cost: 0.277677\n",
      "epoch9200/10000 hypothesis: tensor([152.7898, 184.5706, 179.3298, 196.2908, 142.1261])  cost: 0.271567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9300/10000 hypothesis: tensor([152.7822, 184.5714, 179.3401, 196.2824, 142.1325])  cost: 0.265675\n",
      "epoch9400/10000 hypothesis: tensor([152.7747, 184.5722, 179.3501, 196.2742, 142.1388])  cost: 0.259999\n",
      "epoch9500/10000 hypothesis: tensor([152.7673, 184.5730, 179.3599, 196.2661, 142.1450])  cost: 0.254532\n",
      "epoch9600/10000 hypothesis: tensor([152.7600, 184.5738, 179.3695, 196.2581, 142.1511])  cost: 0.249272\n",
      "epoch9700/10000 hypothesis: tensor([152.7529, 184.5746, 179.3788, 196.2503, 142.1572])  cost: 0.244193\n",
      "epoch9800/10000 hypothesis: tensor([152.7458, 184.5754, 179.3880, 196.2427, 142.1631])  cost: 0.239307\n",
      "epoch9900/10000 hypothesis: tensor([152.7389, 184.5762, 179.3970, 196.2352, 142.1689])  cost: 0.234608\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hypothesis = x_train @ W + b\n",
    "\n",
    "    loss = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch{:4d}/{} hypothesis: {}  cost: {:.6f}'.format(\n",
    "            epoch, epochs, hypothesis.squeeze().detach(), loss.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1349],\n",
      "        [0.6806],\n",
      "        [0.2057]], requires_grad=True) tensor([0.0083], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[152.7321],\n",
       "        [184.5770],\n",
       "        [179.4058],\n",
       "        [196.2279],\n",
       "        [142.1747]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train@W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
