{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MyDataset\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.x = torch.FloatTensor([[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]])\n",
    "\n",
    "        self.y = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
    "        #self.y_onehot = torch.zeros(8, 3)\n",
    "        #self.y_onehot.scatter_(1, self.y.unsqueeze(1), 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor(self.x[index])\n",
    "        y = torch.tensor(self.y[index], dtype = torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxClassifierModule(nn.Module):\n",
    "    \"\"\"Some Information about SoftMaxClassifierModule\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SoftMaxClassifierModule, self).__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftMaxClassifierModule()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_888/3916825992.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.y[index], dtype = torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0/1000, batch 0, prediction tensor([[-0.2987, -4.4220, -6.7905]], grad_fn=<AddmmBackward0>), loss 0.017553454264998436\n",
      "epoch    0/1000, batch 1, prediction tensor([[-1.7797, -1.4957, -2.6469]], grad_fn=<AddmmBackward0>), loss 0.7270499467849731\n",
      "epoch    0/1000, batch 2, prediction tensor([[-0.2163, -1.1077, -1.2733]], grad_fn=<AddmmBackward0>), loss 1.620926856994629\n",
      "epoch    0/1000, batch 3, prediction tensor([[-1.4816, -1.1665, -1.8187]], grad_fn=<AddmmBackward0>), loss 1.4633698463439941\n",
      "epoch    0/1000, batch 4, prediction tensor([[-0.2403, -4.0711, -5.1631]], grad_fn=<AddmmBackward0>), loss 3.859363079071045\n",
      "epoch    0/1000, batch 5, prediction tensor([[-1.6197, -0.4545, -1.4181]], grad_fn=<AddmmBackward0>), loss 1.490325689315796\n",
      "epoch    0/1000, batch 6, prediction tensor([[-1.9620, -2.9647, -5.0054]], grad_fn=<AddmmBackward0>), loss 0.3468362092971802\n",
      "epoch    0/1000, batch 7, prediction tensor([[-2.3209, -1.7078, -3.1607]], grad_fn=<AddmmBackward0>), loss 0.5741360783576965\n",
      "epoch   20/1000, batch 0, prediction tensor([[-1.6875, -3.4664, -4.3206]], grad_fn=<AddmmBackward0>), loss 1.9946041107177734\n",
      "epoch   20/1000, batch 1, prediction tensor([[-3.0781, -1.0216, -3.0897]], grad_fn=<AddmmBackward0>), loss 0.22658981382846832\n",
      "epoch   20/1000, batch 2, prediction tensor([[-3.4689, -2.4048, -5.6375]], grad_fn=<AddmmBackward0>), loss 1.389444351196289\n",
      "epoch   20/1000, batch 3, prediction tensor([[-2.1403, -3.0151, -4.7766]], grad_fn=<AddmmBackward0>), loss 0.3978208005428314\n",
      "epoch   20/1000, batch 4, prediction tensor([[-0.8730, -1.1458, -0.5785]], grad_fn=<AddmmBackward0>), loss 0.8381041884422302\n",
      "epoch   20/1000, batch 5, prediction tensor([[-1.8746, -0.8784, -0.7393]], grad_fn=<AddmmBackward0>), loss 0.7845604419708252\n",
      "epoch   20/1000, batch 6, prediction tensor([[-2.4340, -1.4338, -0.5989]], grad_fn=<AddmmBackward0>), loss 0.4659360945224762\n",
      "epoch   20/1000, batch 7, prediction tensor([[-3.3278, -1.7289, -0.8656]], grad_fn=<AddmmBackward0>), loss 1.2734705209732056\n",
      "epoch   40/1000, batch 0, prediction tensor([[-3.5710, -1.3140, -1.0373]], grad_fn=<AddmmBackward0>), loss 0.8851519227027893\n",
      "epoch   40/1000, batch 1, prediction tensor([[-2.1664, -1.3088, -3.7143]], grad_fn=<AddmmBackward0>), loss 0.41501477360725403\n",
      "epoch   40/1000, batch 2, prediction tensor([[-2.3753, -3.0250, -6.1109]], grad_fn=<AddmmBackward0>), loss 0.43570712208747864\n",
      "epoch   40/1000, batch 3, prediction tensor([[-1.7464, -3.0608, -5.1249]], grad_fn=<AddmmBackward0>), loss 0.26447200775146484\n",
      "epoch   40/1000, batch 4, prediction tensor([[-1.0692, -1.2131, -0.3151]], grad_fn=<AddmmBackward0>), loss 0.6301273107528687\n",
      "epoch   40/1000, batch 5, prediction tensor([[-1.2596, -3.7363, -4.4785]], grad_fn=<AddmmBackward0>), loss 2.5935983657836914\n",
      "epoch   40/1000, batch 6, prediction tensor([[-2.9877, -0.6942, -0.7848]], grad_fn=<AddmmBackward0>), loss 0.7908740043640137\n",
      "epoch   40/1000, batch 7, prediction tensor([[-2.3094, -0.6224, -0.5605]], grad_fn=<AddmmBackward0>), loss 0.7485495805740356\n",
      "epoch   60/1000, batch 0, prediction tensor([[-2.5137, -2.8199, -4.1409]], grad_fn=<AddmmBackward0>), loss 0.9651810526847839\n",
      "epoch   60/1000, batch 1, prediction tensor([[-3.5828, -0.4382, -0.4457]], grad_fn=<AddmmBackward0>), loss 0.7182873487472534\n",
      "epoch   60/1000, batch 2, prediction tensor([[-2.7271, -0.4880, -0.2772]], grad_fn=<AddmmBackward0>), loss 0.6398715376853943\n",
      "epoch   60/1000, batch 3, prediction tensor([[-1.7456, -0.9796,  0.1278]], grad_fn=<AddmmBackward0>), loss 0.39473050832748413\n",
      "epoch   60/1000, batch 4, prediction tensor([[-3.6265, -2.5630, -5.3217]], grad_fn=<AddmmBackward0>), loss 1.406046748161316\n",
      "epoch   60/1000, batch 5, prediction tensor([[-2.2480, -1.5771, -3.3643]], grad_fn=<AddmmBackward0>), loss 0.5180060267448425\n",
      "epoch   60/1000, batch 6, prediction tensor([[-2.5831, -2.8287, -4.5204]], grad_fn=<AddmmBackward0>), loss 0.6556307673454285\n",
      "epoch   60/1000, batch 7, prediction tensor([[-3.8877, -1.3167, -0.7179]], grad_fn=<AddmmBackward0>), loss 1.063435435295105\n",
      "epoch   80/1000, batch 0, prediction tensor([[-2.5868, -0.6427, -0.2627]], grad_fn=<AddmmBackward0>), loss 0.5775830149650574\n",
      "epoch   80/1000, batch 1, prediction tensor([[-4.3703, -0.7776, -0.7744]], grad_fn=<AddmmBackward0>), loss 0.7083702683448792\n",
      "epoch   80/1000, batch 2, prediction tensor([[-2.3749, -2.7630, -4.3366]], grad_fn=<AddmmBackward0>), loss 0.9864001274108887\n",
      "epoch   80/1000, batch 3, prediction tensor([[-3.2829, -1.8545, -6.3738]], grad_fn=<AddmmBackward0>), loss 1.6519827842712402\n",
      "epoch   80/1000, batch 4, prediction tensor([[-1.9294, -1.0561, -4.2039]], grad_fn=<AddmmBackward0>), loss 0.3788049519062042\n",
      "epoch   80/1000, batch 5, prediction tensor([[-3.3751, -0.6042, -0.4875]], grad_fn=<AddmmBackward0>), loss 0.6655463576316833\n",
      "epoch   80/1000, batch 6, prediction tensor([[-2.2456, -2.6447, -5.0418]], grad_fn=<AddmmBackward0>), loss 0.5492560267448425\n",
      "epoch   80/1000, batch 7, prediction tensor([[-1.5753, -1.2441,  0.2220]], grad_fn=<AddmmBackward0>), loss 0.3340332806110382\n",
      "epoch  100/1000, batch 0, prediction tensor([[-2.0283, -3.2366, -4.2096]], grad_fn=<AddmmBackward0>), loss 1.5529592037200928\n",
      "epoch  100/1000, batch 1, prediction tensor([[-2.4088, -0.5184, -4.2622]], grad_fn=<AddmmBackward0>), loss 0.16099555790424347\n",
      "epoch  100/1000, batch 2, prediction tensor([[-3.8734, -0.3396, -0.2537]], grad_fn=<AddmmBackward0>), loss 0.6650111079216003\n",
      "epoch  100/1000, batch 3, prediction tensor([[-2.8580, -2.0813, -4.9928]], grad_fn=<AddmmBackward0>), loss 1.1916249990463257\n",
      "epoch  100/1000, batch 4, prediction tensor([[-4.3329, -0.8622, -0.7273]], grad_fn=<AddmmBackward0>), loss 0.7772697806358337\n",
      "epoch  100/1000, batch 5, prediction tensor([[-1.8191, -1.1264,  0.3481]], grad_fn=<AddmmBackward0>), loss 0.29517674446105957\n",
      "epoch  100/1000, batch 6, prediction tensor([[-2.1933, -2.8168, -6.5010]], grad_fn=<AddmmBackward0>), loss 0.4379657506942749\n",
      "epoch  100/1000, batch 7, prediction tensor([[-2.4426, -0.7952, -0.2545]], grad_fn=<AddmmBackward0>), loss 0.5273625254631042\n",
      "epoch  120/1000, batch 0, prediction tensor([[-1.8595, -0.7637, -4.5662]], grad_fn=<AddmmBackward0>), loss 0.30497467517852783\n",
      "epoch  120/1000, batch 1, prediction tensor([[-2.2475, -2.2088, -5.4758]], grad_fn=<AddmmBackward0>), loss 0.7319705486297607\n",
      "epoch  120/1000, batch 2, prediction tensor([[-1.7898, -3.2552, -4.4295]], grad_fn=<AddmmBackward0>), loss 1.7296291589736938\n",
      "epoch  120/1000, batch 3, prediction tensor([[-3.8813, -0.2929, -0.2925]], grad_fn=<AddmmBackward0>), loss 0.7066760063171387\n",
      "epoch  120/1000, batch 4, prediction tensor([[-2.8268, -0.5910, -0.0745]], grad_fn=<AddmmBackward0>), loss 0.507054328918457\n",
      "epoch  120/1000, batch 5, prediction tensor([[-2.6029, -2.6123, -6.2960]], grad_fn=<AddmmBackward0>), loss 0.7008647322654724\n",
      "epoch  120/1000, batch 6, prediction tensor([[-4.3478, -0.8858, -0.6886]], grad_fn=<AddmmBackward0>), loss 0.810644268989563\n",
      "epoch  120/1000, batch 7, prediction tensor([[-1.9169, -1.1846,  0.5041]], grad_fn=<AddmmBackward0>), loss 0.2418520152568817\n",
      "epoch  140/1000, batch 0, prediction tensor([[-2.4268, -2.4471, -6.6373]], grad_fn=<AddmmBackward0>), loss 0.6905336976051331\n",
      "epoch  140/1000, batch 1, prediction tensor([[-4.3751, -0.7415, -0.8057]], grad_fn=<AddmmBackward0>), loss 0.6751381754875183\n",
      "epoch  140/1000, batch 2, prediction tensor([[-2.6402, -0.6712, -0.1808]], grad_fn=<AddmmBackward0>), loss 0.5293691754341125\n",
      "epoch  140/1000, batch 3, prediction tensor([[-3.6951, -0.5888, -0.1828]], grad_fn=<AddmmBackward0>), loss 0.5283538699150085\n",
      "epoch  140/1000, batch 4, prediction tensor([[-1.7642, -2.7857, -5.3822]], grad_fn=<AddmmBackward0>), loss 0.32705605030059814\n",
      "epoch  140/1000, batch 5, prediction tensor([[-1.9620, -1.3361,  0.7007]], grad_fn=<AddmmBackward0>), loss 0.18250149488449097\n",
      "epoch  140/1000, batch 6, prediction tensor([[-1.3478, -1.2827, -4.5589]], grad_fn=<AddmmBackward0>), loss 0.6804319024085999\n",
      "epoch  140/1000, batch 7, prediction tensor([[-2.0411, -3.1983, -4.2350]], grad_fn=<AddmmBackward0>), loss 1.5120086669921875\n",
      "epoch  160/1000, batch 0, prediction tensor([[-2.6539, -0.9046,  0.0662]], grad_fn=<AddmmBackward0>), loss 0.3678576350212097\n",
      "epoch  160/1000, batch 1, prediction tensor([[-1.3815, -1.1718, -4.6361]], grad_fn=<AddmmBackward0>), loss 0.6109179258346558\n",
      "epoch  160/1000, batch 2, prediction tensor([[-1.8798, -2.6365, -5.4158]], grad_fn=<AddmmBackward0>), loss 0.4043728709220886\n",
      "epoch  160/1000, batch 3, prediction tensor([[-2.0839, -1.3537,  0.8402]], grad_fn=<AddmmBackward0>), loss 0.15289151668548584\n",
      "epoch  160/1000, batch 4, prediction tensor([[-1.4296, -3.4256, -6.6560]], grad_fn=<AddmmBackward0>), loss 0.1321197748184204\n",
      "epoch  160/1000, batch 5, prediction tensor([[-4.3783, -0.9039, -0.6402]], grad_fn=<AddmmBackward0>), loss 0.8470172882080078\n",
      "epoch  160/1000, batch 6, prediction tensor([[-3.7413, -0.5646, -0.1608]], grad_fn=<AddmmBackward0>), loss 0.5280569791793823\n",
      "epoch  160/1000, batch 7, prediction tensor([[-1.6483, -3.4662, -4.3600]], grad_fn=<AddmmBackward0>), loss 2.0240092277526855\n",
      "epoch  180/1000, batch 0, prediction tensor([[-2.2742, -2.6527, -4.5475]], grad_fn=<AddmmBackward0>), loss 0.9595201015472412\n",
      "epoch  180/1000, batch 1, prediction tensor([[-4.3810, -0.0235, -0.0623]], grad_fn=<AddmmBackward0>), loss 0.7192315459251404\n",
      "epoch  180/1000, batch 2, prediction tensor([[-3.0758, -0.5477,  0.1312]], grad_fn=<AddmmBackward0>), loss 0.43674811720848083\n",
      "epoch  180/1000, batch 3, prediction tensor([[-2.0510, -0.3766, -4.7618]], grad_fn=<AddmmBackward0>), loss 0.18221907317638397\n",
      "epoch  180/1000, batch 4, prediction tensor([[-2.6731, -1.7371, -5.5219]], grad_fn=<AddmmBackward0>), loss 1.2830816507339478\n",
      "epoch  180/1000, batch 5, prediction tensor([[-1.8205, -2.8490, -6.8417]], grad_fn=<AddmmBackward0>), loss 0.31053146719932556\n",
      "epoch  180/1000, batch 6, prediction tensor([[-4.5486, -0.7849, -0.5888]], grad_fn=<AddmmBackward0>), loss 0.8064033389091492\n",
      "epoch  180/1000, batch 7, prediction tensor([[-2.2249, -1.2340,  0.8614]], grad_fn=<AddmmBackward0>), loss 0.15588292479515076\n",
      "epoch  200/1000, batch 0, prediction tensor([[-1.4482, -2.9530, -7.1100]], grad_fn=<AddmmBackward0>), loss 0.20339590311050415\n",
      "epoch  200/1000, batch 1, prediction tensor([[-1.3387, -2.8364, -5.7570]], grad_fn=<AddmmBackward0>), loss 0.21164880692958832\n",
      "epoch  200/1000, batch 2, prediction tensor([[-4.4239, -0.7877, -0.7107]], grad_fn=<AddmmBackward0>), loss 0.7449731826782227\n",
      "epoch  200/1000, batch 3, prediction tensor([[-2.2407, -1.2993,  0.9427]], grad_fn=<AddmmBackward0>), loss 0.13775230944156647\n",
      "epoch  200/1000, batch 4, prediction tensor([[-2.6655, -0.8228, -0.0040]], grad_fn=<AddmmBackward0>), loss 0.41264957189559937\n",
      "epoch  200/1000, batch 5, prediction tensor([[-1.0379, -0.9248, -5.2267]], grad_fn=<AddmmBackward0>), loss 0.6453042030334473\n",
      "epoch  200/1000, batch 6, prediction tensor([[-4.1238e+00, -3.4370e-01,  7.2467e-04]], grad_fn=<AddmmBackward0>), loss 0.54510897397995\n",
      "epoch  200/1000, batch 7, prediction tensor([[-1.9269, -3.1662, -4.3814]], grad_fn=<AddmmBackward0>), loss 1.558074951171875\n",
      "epoch  220/1000, batch 0, prediction tensor([[-4.7235, -0.5682, -0.6306]], grad_fn=<AddmmBackward0>), loss 0.6704828143119812\n",
      "epoch  220/1000, batch 1, prediction tensor([[-1.1956, -0.5889, -5.4049]], grad_fn=<AddmmBackward0>), loss 0.440329909324646\n",
      "epoch  220/1000, batch 2, prediction tensor([[-1.6477, -2.2942, -7.5693]], grad_fn=<AddmmBackward0>), loss 0.4230238199234009\n",
      "epoch  220/1000, batch 3, prediction tensor([[-1.7995, -3.0103, -4.6647]], grad_fn=<AddmmBackward0>), loss 1.5145742893218994\n",
      "epoch  220/1000, batch 4, prediction tensor([[-4.4651e+00,  2.2152e-03, -3.9028e-03]], grad_fn=<AddmmBackward0>), loss 0.7019511461257935\n",
      "epoch  220/1000, batch 5, prediction tensor([[-2.0904, -1.9254, -5.9164]], grad_fn=<AddmmBackward0>), loss 0.788988471031189\n",
      "epoch  220/1000, batch 6, prediction tensor([[-2.8353, -0.8374,  0.1804]], grad_fn=<AddmmBackward0>), loss 0.34388479590415955\n",
      "epoch  220/1000, batch 7, prediction tensor([[-2.4513, -1.2943,  1.1481]], grad_fn=<AddmmBackward0>), loss 0.10822192579507828\n",
      "epoch  240/1000, batch 0, prediction tensor([[-2.1486, -1.8282, -5.9553]], grad_fn=<AddmmBackward0>), loss 0.8754122853279114\n",
      "epoch  240/1000, batch 1, prediction tensor([[-2.0003, -3.0004, -4.4738]], grad_fn=<AddmmBackward0>), loss 1.3731296062469482\n",
      "epoch  240/1000, batch 2, prediction tensor([[-1.7650, -0.1056, -5.3187]], grad_fn=<AddmmBackward0>), loss 0.17872345447540283\n",
      "epoch  240/1000, batch 3, prediction tensor([[-3.2294, -0.4980,  0.2351]], grad_fn=<AddmmBackward0>), loss 0.4132380187511444\n",
      "epoch  240/1000, batch 4, prediction tensor([[-2.3246, -1.8672, -7.3194]], grad_fn=<AddmmBackward0>), loss 0.9503508806228638\n",
      "epoch  240/1000, batch 5, prediction tensor([[-2.5561, -1.2821,  1.2407]], grad_fn=<AddmmBackward0>), loss 0.09774063527584076\n",
      "epoch  240/1000, batch 6, prediction tensor([[-4.9985, -0.3614, -0.5624]], grad_fn=<AddmmBackward0>), loss 0.6030169129371643\n",
      "epoch  240/1000, batch 7, prediction tensor([[-4.4302, -0.1646,  0.1281]], grad_fn=<AddmmBackward0>), loss 0.5634358525276184\n",
      "epoch  260/1000, batch 0, prediction tensor([[-4.5702, -0.2122,  0.3157]], grad_fn=<AddmmBackward0>), loss 0.4683581590652466\n",
      "epoch  260/1000, batch 1, prediction tensor([[-2.1145, -2.9981, -4.3619]], grad_fn=<AddmmBackward0>), loss 1.3016180992126465\n",
      "epoch  260/1000, batch 2, prediction tensor([[-2.1907, -1.9003, -7.4201]], grad_fn=<AddmmBackward0>), loss 0.8511659502983093\n",
      "epoch  260/1000, batch 3, prediction tensor([[-2.6487, -1.2841,  1.3354]], grad_fn=<AddmmBackward0>), loss 0.08750935643911362\n",
      "epoch  260/1000, batch 4, prediction tensor([[-5.0650, -0.3420, -0.5153]], grad_fn=<AddmmBackward0>), loss 0.6150757670402527\n",
      "epoch  260/1000, batch 5, prediction tensor([[-1.5321, -2.1284, -6.2716]], grad_fn=<AddmmBackward0>), loss 0.44445550441741943\n",
      "epoch  260/1000, batch 6, prediction tensor([[-0.9552, -0.6627, -5.5715]], grad_fn=<AddmmBackward0>), loss 0.5617924928665161\n",
      "epoch  260/1000, batch 7, prediction tensor([[-2.9924, -0.6931,  0.1932]], grad_fn=<AddmmBackward0>), loss 0.3740052878856659\n",
      "epoch  280/1000, batch 0, prediction tensor([[-1.1109, -2.5544, -7.8459]], grad_fn=<AddmmBackward0>), loss 0.21292775869369507\n",
      "epoch  280/1000, batch 1, prediction tensor([[-2.8963, -0.8546,  0.2586]], grad_fn=<AddmmBackward0>), loss 0.3156425952911377\n",
      "epoch  280/1000, batch 2, prediction tensor([[-4.4714, -0.3536,  0.3583]], grad_fn=<AddmmBackward0>), loss 0.40457621216773987\n",
      "epoch  280/1000, batch 3, prediction tensor([[-2.6437, -1.3780,  1.4243]], grad_fn=<AddmmBackward0>), loss 0.07490791380405426\n",
      "epoch  280/1000, batch 4, prediction tensor([[-1.1169, -2.7908, -6.0244]], grad_fn=<AddmmBackward0>), loss 0.17807959020137787\n",
      "epoch  280/1000, batch 5, prediction tensor([[-4.7987, -0.6326, -0.4910]], grad_fn=<AddmmBackward0>), loss 0.773679256439209\n",
      "epoch  280/1000, batch 6, prediction tensor([[-0.8017, -0.6722, -5.7155]], grad_fn=<AddmmBackward0>), loss 0.6339380741119385\n",
      "epoch  280/1000, batch 7, prediction tensor([[-1.9104, -2.7921, -4.7720]], grad_fn=<AddmmBackward0>), loss 1.2678190469741821\n",
      "epoch  300/1000, batch 0, prediction tensor([[-1.3758, -2.3247, -6.2316]], grad_fn=<AddmmBackward0>), loss 0.33287644386291504\n",
      "epoch  300/1000, batch 1, prediction tensor([[-5.0440, -0.2821, -0.5962]], grad_fn=<AddmmBackward0>), loss 0.5533174276351929\n",
      "epoch  300/1000, batch 2, prediction tensor([[-0.7740, -2.6249, -8.1123]], grad_fn=<AddmmBackward0>), loss 0.1464787870645523\n",
      "epoch  300/1000, batch 3, prediction tensor([[-2.8997, -0.8112,  0.2186]], grad_fn=<AddmmBackward0>), loss 0.33740103244781494\n",
      "epoch  300/1000, batch 4, prediction tensor([[-4.5497, -0.2334,  0.3163]], grad_fn=<AddmmBackward0>), loss 0.4604622721672058\n",
      "epoch  300/1000, batch 5, prediction tensor([[-1.5671, -3.3395, -4.5679]], grad_fn=<AddmmBackward0>), loss 1.9709124565124512\n",
      "epoch  300/1000, batch 6, prediction tensor([[-2.8918, -1.1785,  1.4728]], grad_fn=<AddmmBackward0>), loss 0.07999212294816971\n",
      "epoch  300/1000, batch 7, prediction tensor([[-1.3940, -0.1861, -5.6093]], grad_fn=<AddmmBackward0>), loss 0.2648635804653168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  320/1000, batch 0, prediction tensor([[-5.8076,  0.3636, -0.4784]], grad_fn=<AddmmBackward0>), loss 0.3597244620323181\n",
      "epoch  320/1000, batch 1, prediction tensor([[-2.7955, -1.9870, -4.6919]], grad_fn=<AddmmBackward0>), loss 0.41369813680648804\n",
      "epoch  320/1000, batch 2, prediction tensor([[-5.3729,  0.4954,  0.4107]], grad_fn=<AddmmBackward0>), loss 0.7378658056259155\n",
      "epoch  320/1000, batch 3, prediction tensor([[-3.4818, -0.4698,  0.4593]], grad_fn=<AddmmBackward0>), loss 0.346655011177063\n",
      "epoch  320/1000, batch 4, prediction tensor([[-2.4544, -1.3965, -6.0812]], grad_fn=<AddmmBackward0>), loss 1.3627086877822876\n",
      "epoch  320/1000, batch 5, prediction tensor([[-2.9521, -1.2639,  1.6187]], grad_fn=<AddmmBackward0>), loss 0.06423118710517883\n",
      "epoch  320/1000, batch 6, prediction tensor([[-1.2817, -0.4339, -5.4738]], grad_fn=<AddmmBackward0>), loss 0.3610292971134186\n",
      "epoch  320/1000, batch 7, prediction tensor([[-1.7005, -2.2112, -7.5994]], grad_fn=<AddmmBackward0>), loss 0.47177040576934814\n",
      "epoch  340/1000, batch 0, prediction tensor([[-5.3638,  0.0120, -0.5705]], grad_fn=<AddmmBackward0>), loss 0.44668421149253845\n",
      "epoch  340/1000, batch 1, prediction tensor([[-0.9400, -2.2495, -8.3217]], grad_fn=<AddmmBackward0>), loss 0.23946236073970795\n",
      "epoch  340/1000, batch 2, prediction tensor([[-3.0252, -0.7626,  0.2956]], grad_fn=<AddmmBackward0>), loss 0.3243900537490845\n",
      "epoch  340/1000, batch 3, prediction tensor([[-0.8333, -0.4322, -5.9238]], grad_fn=<AddmmBackward0>), loss 0.5150462985038757\n",
      "epoch  340/1000, batch 4, prediction tensor([[-2.9413, -1.2478,  1.5917]], grad_fn=<AddmmBackward0>), loss 0.06691230088472366\n",
      "epoch  340/1000, batch 5, prediction tensor([[-5.0234,  0.1049,  0.4518]], grad_fn=<AddmmBackward0>), loss 0.5371106266975403\n",
      "epoch  340/1000, batch 6, prediction tensor([[-1.2980, -2.2852, -6.3489]], grad_fn=<AddmmBackward0>), loss 0.32135000824928284\n",
      "epoch  340/1000, batch 7, prediction tensor([[-1.7183, -3.1606, -4.5955]], grad_fn=<AddmmBackward0>), loss 1.698970079421997\n",
      "epoch  360/1000, batch 0, prediction tensor([[-4.8949, -0.2504,  0.6786]], grad_fn=<AddmmBackward0>), loss 0.3355799615383148\n",
      "epoch  360/1000, batch 1, prediction tensor([[-5.1935, -0.4225, -0.3063]], grad_fn=<AddmmBackward0>), loss 0.7569342255592346\n",
      "epoch  360/1000, batch 2, prediction tensor([[-2.9867, -1.2870,  1.6763]], grad_fn=<AddmmBackward0>), loss 0.05929667502641678\n",
      "epoch  360/1000, batch 3, prediction tensor([[-0.6467, -2.6305, -8.2340]], grad_fn=<AddmmBackward0>), loss 0.12931057810783386\n",
      "epoch  360/1000, batch 4, prediction tensor([[-0.8410, -2.4931, -6.5980]], grad_fn=<AddmmBackward0>), loss 0.177975133061409\n",
      "epoch  360/1000, batch 5, prediction tensor([[-0.4966, -0.7275, -5.9653]], grad_fn=<AddmmBackward0>), loss 0.817550778388977\n",
      "epoch  360/1000, batch 6, prediction tensor([[-3.1027, -0.7878,  0.3983]], grad_fn=<AddmmBackward0>), loss 0.2893441319465637\n",
      "epoch  360/1000, batch 7, prediction tensor([[-1.9188, -2.8684, -4.6873]], grad_fn=<AddmmBackward0>), loss 1.3209280967712402\n",
      "epoch  380/1000, batch 0, prediction tensor([[-0.7823, -2.6161, -8.1127]], grad_fn=<AddmmBackward0>), loss 0.14881442487239838\n",
      "epoch  380/1000, batch 1, prediction tensor([[-1.7457, -3.0875, -4.6413]], grad_fn=<AddmmBackward0>), loss 1.616856575012207\n",
      "epoch  380/1000, batch 2, prediction tensor([[-5.9109,  0.4353, -0.4468]], grad_fn=<AddmmBackward0>), loss 0.34760209918022156\n",
      "epoch  380/1000, batch 3, prediction tensor([[-1.3874,  0.2666, -6.0685]], grad_fn=<AddmmBackward0>), loss 0.17651806771755219\n",
      "epoch  380/1000, batch 4, prediction tensor([[-1.8677, -1.3327, -6.7317]], grad_fn=<AddmmBackward0>), loss 0.9988894462585449\n",
      "epoch  380/1000, batch 5, prediction tensor([[-5.2102,  0.1742,  0.5693]], grad_fn=<AddmmBackward0>), loss 0.5168048739433289\n",
      "epoch  380/1000, batch 6, prediction tensor([[-3.0841, -1.2985,  1.7851]], grad_fn=<AddmmBackward0>), loss 0.05209413170814514\n",
      "epoch  380/1000, batch 7, prediction tensor([[-3.2066, -0.8106,  0.5250]], grad_fn=<AddmmBackward0>), loss 0.252264142036438\n",
      "epoch  400/1000, batch 0, prediction tensor([[-1.4332,  0.3095, -6.0657]], grad_fn=<AddmmBackward0>), loss 0.16276425123214722\n",
      "epoch  400/1000, batch 1, prediction tensor([[-3.3241, -1.0961,  1.8228]], grad_fn=<AddmmBackward0>), loss 0.05809373781085014\n",
      "epoch  400/1000, batch 2, prediction tensor([[-2.6704, -2.0014, -4.8026]], grad_fn=<AddmmBackward0>), loss 0.4529615640640259\n",
      "epoch  400/1000, batch 3, prediction tensor([[-2.2286, -0.9626, -6.7409]], grad_fn=<AddmmBackward0>), loss 1.5167527198791504\n",
      "epoch  400/1000, batch 4, prediction tensor([[-5.4320,  0.2914,  0.6739]], grad_fn=<AddmmBackward0>), loss 0.5213854312896729\n",
      "epoch  400/1000, batch 5, prediction tensor([[-3.3212, -0.7532,  0.5821]], grad_fn=<AddmmBackward0>), loss 0.24940380454063416\n",
      "epoch  400/1000, batch 6, prediction tensor([[-5.8029,  0.1623, -0.2818]], grad_fn=<AddmmBackward0>), loss 0.4971235692501068\n",
      "epoch  400/1000, batch 7, prediction tensor([[-1.0618, -2.0377, -8.4116]], grad_fn=<AddmmBackward0>), loss 0.32026857137680054\n",
      "epoch  420/1000, batch 0, prediction tensor([[-1.4281, -1.6428, -8.4403]], grad_fn=<AddmmBackward0>), loss 0.5920496582984924\n",
      "epoch  420/1000, batch 1, prediction tensor([[-5.7414,  0.3022, -0.4832]], grad_fn=<AddmmBackward0>), loss 0.3772774934768677\n",
      "epoch  420/1000, batch 2, prediction tensor([[-1.1327, -1.8412, -6.9582]], grad_fn=<AddmmBackward0>), loss 0.40235838294029236\n",
      "epoch  420/1000, batch 3, prediction tensor([[-3.1519, -0.7657,  0.4254]], grad_fn=<AddmmBackward0>), loss 0.2865546643733978\n",
      "epoch  420/1000, batch 4, prediction tensor([[-1.6578, -2.9099, -4.9068]], grad_fn=<AddmmBackward0>), loss 1.5333062410354614\n",
      "epoch  420/1000, batch 5, prediction tensor([[-5.6193,  0.4892,  0.6633]], grad_fn=<AddmmBackward0>), loss 0.6108916997909546\n",
      "epoch  420/1000, batch 6, prediction tensor([[-1.2477,  0.0899, -6.0316]], grad_fn=<AddmmBackward0>), loss 0.23480086028575897\n",
      "epoch  420/1000, batch 7, prediction tensor([[-3.3719, -1.1524,  1.9269]], grad_fn=<AddmmBackward0>), loss 0.049728479236364365\n",
      "epoch  440/1000, batch 0, prediction tensor([[-6.1387,  0.5348, -0.3185]], grad_fn=<AddmmBackward0>), loss 0.3557569682598114\n",
      "epoch  440/1000, batch 1, prediction tensor([[-2.5873, -1.9993, -4.8879]], grad_fn=<AddmmBackward0>), loss 0.47692033648490906\n",
      "epoch  440/1000, batch 2, prediction tensor([[-3.6205, -0.3878,  0.5161]], grad_fn=<AddmmBackward0>), loss 0.35132697224617004\n",
      "epoch  440/1000, batch 3, prediction tensor([[-2.0563, -1.0806, -6.7952]], grad_fn=<AddmmBackward0>), loss 1.2979397773742676\n",
      "epoch  440/1000, batch 4, prediction tensor([[-0.9918, -0.0399, -6.1577]], grad_fn=<AddmmBackward0>), loss 0.32799285650253296\n",
      "epoch  440/1000, batch 5, prediction tensor([[-1.1537, -1.8284, -8.5290]], grad_fn=<AddmmBackward0>), loss 0.41206416487693787\n",
      "epoch  440/1000, batch 6, prediction tensor([[-3.2931, -1.2781,  1.9737]], grad_fn=<AddmmBackward0>), loss 0.042930252850055695\n",
      "epoch  440/1000, batch 7, prediction tensor([[-5.4893,  0.1795,  0.8430]], grad_fn=<AddmmBackward0>), loss 0.41663244366645813\n",
      "epoch  460/1000, batch 0, prediction tensor([[-0.8705, -0.1837, -6.1351]], grad_fn=<AddmmBackward0>), loss 0.40932393074035645\n",
      "epoch  460/1000, batch 1, prediction tensor([[-3.4346, -1.2262,  2.0634]], grad_fn=<AddmmBackward0>), loss 0.04053032025694847\n",
      "epoch  460/1000, batch 2, prediction tensor([[-0.9866, -2.0417, -8.4828]], grad_fn=<AddmmBackward0>), loss 0.2991408407688141\n",
      "epoch  460/1000, batch 3, prediction tensor([[-5.6015,  0.1729,  0.9619]], grad_fn=<AddmmBackward0>), loss 0.37549665570259094\n",
      "epoch  460/1000, batch 4, prediction tensor([[-1.0163, -2.3310, -6.5848]], grad_fn=<AddmmBackward0>), loss 0.24087192118167877\n",
      "epoch  460/1000, batch 5, prediction tensor([[-5.6733, -0.0781, -0.1709]], grad_fn=<AddmmBackward0>), loss 0.6497430205345154\n",
      "epoch  460/1000, batch 6, prediction tensor([[-1.7483, -2.8296, -4.8966]], grad_fn=<AddmmBackward0>), loss 1.404897928237915\n",
      "epoch  460/1000, batch 7, prediction tensor([[-3.4818, -0.5579,  0.5474]], grad_fn=<AddmmBackward0>), loss 0.29929623007774353\n",
      "epoch  480/1000, batch 0, prediction tensor([[-3.3468, -0.7647,  0.6193]], grad_fn=<AddmmBackward0>), loss 0.23863552510738373\n",
      "epoch  480/1000, batch 1, prediction tensor([[-5.7076,  0.2479,  0.9930]], grad_fn=<AddmmBackward0>), loss 0.389285147190094\n",
      "epoch  480/1000, batch 2, prediction tensor([[-3.4508, -1.3115,  2.1649]], grad_fn=<AddmmBackward0>), loss 0.03397468477487564\n",
      "epoch  480/1000, batch 3, prediction tensor([[-0.8485, -0.2735, -6.0673]], grad_fn=<AddmmBackward0>), loss 0.4483606815338135\n",
      "epoch  480/1000, batch 4, prediction tensor([[-0.9755, -2.1369, -8.3988]], grad_fn=<AddmmBackward0>), loss 0.2727985382080078\n",
      "epoch  480/1000, batch 5, prediction tensor([[-5.9274,  0.1714, -0.1663]], grad_fn=<AddmmBackward0>), loss 0.5397940278053284\n",
      "epoch  480/1000, batch 6, prediction tensor([[-2.0332, -2.5492, -4.8920]], grad_fn=<AddmmBackward0>), loss 1.0193843841552734\n",
      "epoch  480/1000, batch 7, prediction tensor([[-1.6723, -1.2659, -6.9939]], grad_fn=<AddmmBackward0>), loss 0.9188101887702942\n",
      "epoch  500/1000, batch 0, prediction tensor([[-3.5126, -1.2260,  2.1411]], grad_fn=<AddmmBackward0>), loss 0.037291064858436584\n",
      "epoch  500/1000, batch 1, prediction tensor([[-3.3687, -0.7209,  0.5973]], grad_fn=<AddmmBackward0>), loss 0.25198835134506226\n",
      "epoch  500/1000, batch 2, prediction tensor([[-0.5676, -2.1705, -8.7731]], grad_fn=<AddmmBackward0>), loss 0.18363620340824127\n",
      "epoch  500/1000, batch 3, prediction tensor([[-5.6997,  0.2490,  0.9840]], grad_fn=<AddmmBackward0>), loss 0.39256054162979126\n",
      "epoch  500/1000, batch 4, prediction tensor([[-0.6535, -0.3300, -6.2059]], grad_fn=<AddmmBackward0>), loss 0.5460077524185181\n",
      "epoch  500/1000, batch 5, prediction tensor([[-2.1463, -2.5995, -4.7287]], grad_fn=<AddmmBackward0>), loss 0.9903944730758667\n",
      "epoch  500/1000, batch 6, prediction tensor([[-6.4434,  0.7832, -0.2621]], grad_fn=<AddmmBackward0>), loss 0.3018176257610321\n",
      "epoch  500/1000, batch 7, prediction tensor([[-1.7377, -1.1563, -7.0381]], grad_fn=<AddmmBackward0>), loss 1.0273408889770508\n",
      "epoch  520/1000, batch 0, prediction tensor([[-1.1324,  0.3162, -6.3733]], grad_fn=<AddmmBackward0>), loss 0.21199488639831543\n",
      "epoch  520/1000, batch 1, prediction tensor([[-3.7226, -1.1074,  2.2326]], grad_fn=<AddmmBackward0>), loss 0.03732195869088173\n",
      "epoch  520/1000, batch 2, prediction tensor([[-1.6029, -1.3375, -6.9917]], grad_fn=<AddmmBackward0>), loss 0.8366522789001465\n",
      "epoch  520/1000, batch 3, prediction tensor([[-0.5212, -2.1817, -8.8083]], grad_fn=<AddmmBackward0>), loss 0.1741943508386612\n",
      "epoch  520/1000, batch 4, prediction tensor([[-5.9166,  0.3207, -0.3264]], grad_fn=<AddmmBackward0>), loss 0.4223277270793915\n",
      "epoch  520/1000, batch 5, prediction tensor([[-3.3329, -0.7215,  0.5621]], grad_fn=<AddmmBackward0>), loss 0.2603325843811035\n",
      "epoch  520/1000, batch 6, prediction tensor([[-5.8226,  0.3971,  0.9587]], grad_fn=<AddmmBackward0>), loss 0.45199549198150635\n",
      "epoch  520/1000, batch 7, prediction tensor([[-1.8604, -2.7825, -4.8315]], grad_fn=<AddmmBackward0>), loss 1.2929537296295166\n",
      "epoch  540/1000, batch 0, prediction tensor([[-1.1250, -1.4869, -8.8993]], grad_fn=<AddmmBackward0>), loss 0.5287407040596008\n",
      "epoch  540/1000, batch 1, prediction tensor([[-3.6575, -1.2306,  2.2908]], grad_fn=<AddmmBackward0>), loss 0.03166181966662407\n",
      "epoch  540/1000, batch 2, prediction tensor([[-0.9839, -1.8943, -7.0539]], grad_fn=<AddmmBackward0>), loss 0.3397987484931946\n",
      "epoch  540/1000, batch 3, prediction tensor([[-3.3209, -0.8717,  0.7004]], grad_fn=<AddmmBackward0>), loss 0.20337225496768951\n",
      "epoch  540/1000, batch 4, prediction tensor([[-5.9067,  0.2375, -0.2532]], grad_fn=<AddmmBackward0>), loss 0.47894087433815\n",
      "epoch  540/1000, batch 5, prediction tensor([[-1.8076, -2.6064, -5.0604]], grad_fn=<AddmmBackward0>), loss 1.1965832710266113\n",
      "epoch  540/1000, batch 6, prediction tensor([[-6.1704,  0.7452,  0.9585]], grad_fn=<AddmmBackward0>), loss 0.5926137566566467\n",
      "epoch  540/1000, batch 7, prediction tensor([[-1.0307,  0.2692, -6.4279]], grad_fn=<AddmmBackward0>), loss 0.24199016392230988\n",
      "epoch  560/1000, batch 0, prediction tensor([[-2.4910, -2.0500, -4.9334]], grad_fn=<AddmmBackward0>), loss 0.5302489995956421\n",
      "epoch  560/1000, batch 1, prediction tensor([[-3.9204, -1.0131,  2.3361]], grad_fn=<AddmmBackward0>), loss 0.03636198863387108\n",
      "epoch  560/1000, batch 2, prediction tensor([[-6.3954,  0.7892,  1.1394]], grad_fn=<AddmmBackward0>), loss 0.533586859703064\n",
      "epoch  560/1000, batch 3, prediction tensor([[-1.7191, -1.2674, -6.9457]], grad_fn=<AddmmBackward0>), loss 0.9463742971420288\n",
      "epoch  560/1000, batch 4, prediction tensor([[-6.1296,  0.3333, -0.1261]], grad_fn=<AddmmBackward0>), loss 0.4905344545841217\n",
      "epoch  560/1000, batch 5, prediction tensor([[-0.5758, -1.8368, -9.0986]], grad_fn=<AddmmBackward0>), loss 0.24963833391666412\n",
      "epoch  560/1000, batch 6, prediction tensor([[-0.5342, -0.0772, -6.5779]], grad_fn=<AddmmBackward0>), loss 0.49144482612609863\n",
      "epoch  560/1000, batch 7, prediction tensor([[-3.4783, -0.6845,  0.6706]], grad_fn=<AddmmBackward0>), loss 0.24192988872528076\n",
      "epoch  580/1000, batch 0, prediction tensor([[-0.4666, -2.0064, -9.0382]], grad_fn=<AddmmBackward0>), loss 0.1944359838962555\n",
      "epoch  580/1000, batch 1, prediction tensor([[-3.4234, -0.8084,  0.7395]], grad_fn=<AddmmBackward0>), loss 0.20558983087539673\n",
      "epoch  580/1000, batch 2, prediction tensor([[-6.1234,  0.3976,  1.2590]], grad_fn=<AddmmBackward0>), loss 0.3529168367385864\n",
      "epoch  580/1000, batch 3, prediction tensor([[-1.9283, -2.8253, -4.7209]], grad_fn=<AddmmBackward0>), loss 1.2816072702407837\n",
      "epoch  580/1000, batch 4, prediction tensor([[-1.1029,  0.2885, -6.3750]], grad_fn=<AddmmBackward0>), loss 0.22313763201236725\n",
      "epoch  580/1000, batch 5, prediction tensor([[-3.9473, -1.1064,  2.4563]], grad_fn=<AddmmBackward0>), loss 0.029576482251286507\n",
      "epoch  580/1000, batch 6, prediction tensor([[-1.6104, -1.3519, -6.9698]], grad_fn=<AddmmBackward0>), loss 0.8327547311782837\n",
      "epoch  580/1000, batch 7, prediction tensor([[-6.2833,  0.4515, -0.0905]], grad_fn=<AddmmBackward0>), loss 0.45917534828186035\n",
      "epoch  600/1000, batch 0, prediction tensor([[-3.8728, -1.1693,  2.4447]], grad_fn=<AddmmBackward0>), loss 0.02834499627351761\n",
      "epoch  600/1000, batch 1, prediction tensor([[-2.2199, -2.2397, -5.0148]], grad_fn=<AddmmBackward0>), loss 0.7335177659988403\n",
      "epoch  600/1000, batch 2, prediction tensor([[-1.1308,  0.6058, -6.6643]], grad_fn=<AddmmBackward0>), loss 0.16281156241893768\n",
      "epoch  600/1000, batch 3, prediction tensor([[-1.2320, -1.0421, -9.2371]], grad_fn=<AddmmBackward0>), loss 0.7927981615066528\n",
      "epoch  600/1000, batch 4, prediction tensor([[-6.3051,  0.7090, -0.3262]], grad_fn=<AddmmBackward0>), loss 0.3046015799045563\n",
      "epoch  600/1000, batch 5, prediction tensor([[-3.5134, -0.6080,  0.6292]], grad_fn=<AddmmBackward0>), loss 0.2670277953147888\n",
      "epoch  600/1000, batch 6, prediction tensor([[-0.9364, -1.5799, -7.4158]], grad_fn=<AddmmBackward0>), loss 0.4232991337776184\n",
      "epoch  600/1000, batch 7, prediction tensor([[-6.1211,  0.5124,  1.1420]], grad_fn=<AddmmBackward0>), loss 0.4275752305984497\n",
      "epoch  620/1000, batch 0, prediction tensor([[-0.9042, -1.8774, -7.1504]], grad_fn=<AddmmBackward0>), loss 0.32194042205810547\n",
      "epoch  620/1000, batch 1, prediction tensor([[-1.8797, -2.7299, -4.8648]], grad_fn=<AddmmBackward0>), loss 1.2408006191253662\n",
      "epoch  620/1000, batch 2, prediction tensor([[-6.5792,  0.7939, -0.1370]], grad_fn=<AddmmBackward0>), loss 0.33278194069862366\n",
      "epoch  620/1000, batch 3, prediction tensor([[-0.9563,  0.5001, -6.7332]], grad_fn=<AddmmBackward0>), loss 0.21008487045764923\n",
      "epoch  620/1000, batch 4, prediction tensor([[-6.6314,  0.9196,  1.2450]], grad_fn=<AddmmBackward0>), loss 0.5438627004623413\n",
      "epoch  620/1000, batch 5, prediction tensor([[-3.7217, -0.6054,  0.8348]], grad_fn=<AddmmBackward0>), loss 0.22105492651462555\n",
      "epoch  620/1000, batch 6, prediction tensor([[-1.0206, -1.5043, -8.9863]], grad_fn=<AddmmBackward0>), loss 0.4804779887199402\n",
      "epoch  620/1000, batch 7, prediction tensor([[-3.9432, -1.2204,  2.5663]], grad_fn=<AddmmBackward0>), loss 0.023872558027505875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  640/1000, batch 0, prediction tensor([[-6.4376,  0.5303,  1.4405]], grad_fn=<AddmmBackward0>), loss 0.33848923444747925\n",
      "epoch  640/1000, batch 1, prediction tensor([[-2.1499, -2.5475, -4.7771]], grad_fn=<AddmmBackward0>), loss 0.9538969993591309\n",
      "epoch  640/1000, batch 2, prediction tensor([[-1.0385,  0.3465, -6.4974]], grad_fn=<AddmmBackward0>), loss 0.22425882518291473\n",
      "epoch  640/1000, batch 3, prediction tensor([[-1.5992, -1.2095, -7.1235]], grad_fn=<AddmmBackward0>), loss 0.9084592461585999\n",
      "epoch  640/1000, batch 4, prediction tensor([[-4.0057, -1.2145,  2.6228]], grad_fn=<AddmmBackward0>), loss 0.022615376859903336\n",
      "epoch  640/1000, batch 5, prediction tensor([[-0.4076, -2.0748, -9.0288]], grad_fn=<AddmmBackward0>), loss 0.17307382822036743\n",
      "epoch  640/1000, batch 6, prediction tensor([[-6.2971,  0.3655,  0.0093]], grad_fn=<AddmmBackward0>), loss 0.5315721035003662\n",
      "epoch  640/1000, batch 7, prediction tensor([[-3.4946, -0.7637,  0.7660]], grad_fn=<AddmmBackward0>), loss 0.20758844912052155\n",
      "epoch  660/1000, batch 0, prediction tensor([[-0.3193, -1.8410, -9.3509]], grad_fn=<AddmmBackward0>), loss 0.19758401811122894\n",
      "epoch  660/1000, batch 1, prediction tensor([[-6.4581,  0.5830,  1.4084]], grad_fn=<AddmmBackward0>), loss 0.36357012391090393\n",
      "epoch  660/1000, batch 2, prediction tensor([[-1.9884, -2.6058, -4.8803]], grad_fn=<AddmmBackward0>), loss 1.0841466188430786\n",
      "epoch  660/1000, batch 3, prediction tensor([[-4.1578, -1.0947,  2.6551]], grad_fn=<AddmmBackward0>), loss 0.024322913959622383\n",
      "epoch  660/1000, batch 4, prediction tensor([[-3.7219, -0.6478,  0.8774]], grad_fn=<AddmmBackward0>), loss 0.2050795704126358\n",
      "epoch  660/1000, batch 5, prediction tensor([[-6.7308e+00,  8.1359e-01, -5.1641e-03]], grad_fn=<AddmmBackward0>), loss 0.3656911551952362\n",
      "epoch  660/1000, batch 6, prediction tensor([[-0.9505,  0.5190, -6.7579]], grad_fn=<AddmmBackward0>), loss 0.2076156735420227\n",
      "epoch  660/1000, batch 7, prediction tensor([[-1.4526, -1.0738, -7.4057]], grad_fn=<AddmmBackward0>), loss 0.9014082551002502\n",
      "epoch  680/1000, batch 0, prediction tensor([[ 0.0279, -2.1313, -9.4078]], grad_fn=<AddmmBackward0>), loss 0.10929897427558899\n",
      "epoch  680/1000, batch 1, prediction tensor([[-0.3595, -0.0532, -6.7767]], grad_fn=<AddmmBackward0>), loss 0.5523912906646729\n",
      "epoch  680/1000, batch 2, prediction tensor([[-6.5516,  0.7946, -0.1653]], grad_fn=<AddmmBackward0>), loss 0.3246586322784424\n",
      "epoch  680/1000, batch 3, prediction tensor([[-2.1405, -2.1293, -5.2046]], grad_fn=<AddmmBackward0>), loss 0.7105212807655334\n",
      "epoch  680/1000, batch 4, prediction tensor([[-1.3000, -0.9832, -7.6489]], grad_fn=<AddmmBackward0>), loss 0.864811897277832\n",
      "epoch  680/1000, batch 5, prediction tensor([[-4.0642, -1.1579,  2.6247]], grad_fn=<AddmmBackward0>), loss 0.023723093792796135\n",
      "epoch  680/1000, batch 6, prediction tensor([[-3.5449, -0.6815,  0.7342]], grad_fn=<AddmmBackward0>), loss 0.2284303605556488\n",
      "epoch  680/1000, batch 7, prediction tensor([[-6.5835,  0.7415,  1.3753]], grad_fn=<AddmmBackward0>), loss 0.42587044835090637\n",
      "epoch  700/1000, batch 0, prediction tensor([[-4.1025, -1.2362,  2.7412]], grad_fn=<AddmmBackward0>), loss 0.019606243818998337\n",
      "epoch  700/1000, batch 1, prediction tensor([[ 0.0142, -2.1195, -9.4059]], grad_fn=<AddmmBackward0>), loss 0.11196313053369522\n",
      "epoch  700/1000, batch 2, prediction tensor([[-3.5310, -0.8517,  0.8904]], grad_fn=<AddmmBackward0>), loss 0.1715676337480545\n",
      "epoch  700/1000, batch 3, prediction tensor([[-6.4598,  0.5529, -0.0155]], grad_fn=<AddmmBackward0>), loss 0.44939088821411133\n",
      "epoch  700/1000, batch 4, prediction tensor([[-1.8555, -2.4475, -5.1715]], grad_fn=<AddmmBackward0>), loss 1.055432915687561\n",
      "epoch  700/1000, batch 5, prediction tensor([[-6.9226,  1.0346,  1.4212]], grad_fn=<AddmmBackward0>), loss 0.518524169921875\n",
      "epoch  700/1000, batch 6, prediction tensor([[-1.1329, -1.3742, -7.4250]], grad_fn=<AddmmBackward0>), loss 0.5807774066925049\n",
      "epoch  700/1000, batch 7, prediction tensor([[-0.5380,  0.1160, -6.7674]], grad_fn=<AddmmBackward0>), loss 0.41936787962913513\n",
      "epoch  720/1000, batch 0, prediction tensor([[-6.6516,  0.7970, -0.0678]], grad_fn=<AddmmBackward0>), loss 0.351848304271698\n",
      "epoch  720/1000, batch 1, prediction tensor([[-4.1936, -1.1479,  2.7441]], grad_fn=<AddmmBackward0>), loss 0.021150168031454086\n",
      "epoch  720/1000, batch 2, prediction tensor([[-3.6329, -0.6574,  0.7980]], grad_fn=<AddmmBackward0>), loss 0.21931485831737518\n",
      "epoch  720/1000, batch 3, prediction tensor([[-0.0934, -1.7997, -9.6181]], grad_fn=<AddmmBackward0>), loss 0.16688981652259827\n",
      "epoch  720/1000, batch 4, prediction tensor([[-0.4470,  0.1846, -6.9270]], grad_fn=<AddmmBackward0>), loss 0.4269241988658905\n",
      "epoch  720/1000, batch 5, prediction tensor([[-2.1776, -2.1733, -5.1236]], grad_fn=<AddmmBackward0>), loss 0.7168897390365601\n",
      "epoch  720/1000, batch 6, prediction tensor([[-7.1088,  1.1410,  1.5010]], grad_fn=<AddmmBackward0>), loss 0.5293715596199036\n",
      "epoch  720/1000, batch 7, prediction tensor([[-1.2978, -1.2301, -7.4042]], grad_fn=<AddmmBackward0>), loss 0.7286602854728699\n",
      "epoch  740/1000, batch 0, prediction tensor([[-6.8806,  0.7381,  1.6757]], grad_fn=<AddmmBackward0>), loss 0.3305692970752716\n",
      "epoch  740/1000, batch 1, prediction tensor([[-4.2188, -1.2675,  2.8889]], grad_fn=<AddmmBackward0>), loss 0.016349049285054207\n",
      "epoch  740/1000, batch 2, prediction tensor([[ 0.0572, -2.2449, -9.3235]], grad_fn=<AddmmBackward0>), loss 0.09542658925056458\n",
      "epoch  740/1000, batch 3, prediction tensor([[-1.9052, -2.7008, -4.8685]], grad_fn=<AddmmBackward0>), loss 1.2030504941940308\n",
      "epoch  740/1000, batch 4, prediction tensor([[-3.8227, -0.6612,  0.9916]], grad_fn=<AddmmBackward0>), loss 0.1820145845413208\n",
      "epoch  740/1000, batch 5, prediction tensor([[-1.1689, -1.4660, -7.2972]], grad_fn=<AddmmBackward0>), loss 0.5568065047264099\n",
      "epoch  740/1000, batch 6, prediction tensor([[-6.7180,  0.6367,  0.1589]], grad_fn=<AddmmBackward0>), loss 0.4829038381576538\n",
      "epoch  740/1000, batch 7, prediction tensor([[-0.5816,  0.2960, -6.9038]], grad_fn=<AddmmBackward0>), loss 0.3482098877429962\n",
      "epoch  760/1000, batch 0, prediction tensor([[-4.2954, -1.1939,  2.8919]], grad_fn=<AddmmBackward0>), loss 0.01741393655538559\n",
      "epoch  760/1000, batch 1, prediction tensor([[-0.6366, -1.7353, -7.5603]], grad_fn=<AddmmBackward0>), loss 0.28840523958206177\n",
      "epoch  760/1000, batch 2, prediction tensor([[ 0.3350, -2.2406, -9.6056]], grad_fn=<AddmmBackward0>), loss 0.07339423149824142\n",
      "epoch  760/1000, batch 3, prediction tensor([[-0.2844, -0.0097, -6.8953]], grad_fn=<AddmmBackward0>), loss 0.5657961964607239\n",
      "epoch  760/1000, batch 4, prediction tensor([[-7.0069,  0.8389,  1.7012]], grad_fn=<AddmmBackward0>), loss 0.35233116149902344\n",
      "epoch  760/1000, batch 5, prediction tensor([[-2.0563, -2.5007, -4.9174]], grad_fn=<AddmmBackward0>), loss 0.9740803837776184\n",
      "epoch  760/1000, batch 6, prediction tensor([[-7.1600,  1.1417,  0.0960]], grad_fn=<AddmmBackward0>), loss 0.3013395667076111\n",
      "epoch  760/1000, batch 7, prediction tensor([[-3.8837, -0.5116,  0.9031]], grad_fn=<AddmmBackward0>), loss 0.22421415150165558\n",
      "epoch  780/1000, batch 0, prediction tensor([[-0.7838, -1.6425, -7.5059]], grad_fn=<AddmmBackward0>), loss 0.3541131317615509\n",
      "epoch  780/1000, batch 1, prediction tensor([[-1.9307, -2.5618, -4.9819]], grad_fn=<AddmmBackward0>), loss 1.0881004333496094\n",
      "epoch  780/1000, batch 2, prediction tensor([[-7.3190,  1.0533,  1.7989]], grad_fn=<AddmmBackward0>), loss 0.38833361864089966\n",
      "epoch  780/1000, batch 3, prediction tensor([[-7.1585,  1.0152,  0.2210]], grad_fn=<AddmmBackward0>), loss 0.3730740249156952\n",
      "epoch  780/1000, batch 4, prediction tensor([[-0.9122,  0.6440, -6.9213]], grad_fn=<AddmmBackward0>), loss 0.19182299077510834\n",
      "epoch  780/1000, batch 5, prediction tensor([[-0.6929, -1.1661, -9.6522]], grad_fn=<AddmmBackward0>), loss 0.48435738682746887\n",
      "epoch  780/1000, batch 6, prediction tensor([[-4.3826, -1.1617,  2.9469]], grad_fn=<AddmmBackward0>), loss 0.016942642629146576\n",
      "epoch  780/1000, batch 7, prediction tensor([[-3.7545, -0.6962,  0.9584]], grad_fn=<AddmmBackward0>), loss 0.18244348466396332\n",
      "epoch  800/1000, batch 0, prediction tensor([[-7.1760,  1.1823,  0.0714]], grad_fn=<AddmmBackward0>), loss 0.28480738401412964\n",
      "epoch  800/1000, batch 1, prediction tensor([[-7.3717,  1.1891,  1.7159]], grad_fn=<AddmmBackward0>), loss 0.4641081690788269\n",
      "epoch  800/1000, batch 2, prediction tensor([[-0.8610,  0.5820, -6.9104]], grad_fn=<AddmmBackward0>), loss 0.21251878142356873\n",
      "epoch  800/1000, batch 3, prediction tensor([[-2.6685, -1.7795, -5.0265]], grad_fn=<AddmmBackward0>), loss 0.37151145935058594\n",
      "epoch  800/1000, batch 4, prediction tensor([[-1.4936, -0.8292, -7.6093]], grad_fn=<AddmmBackward0>), loss 1.0802719593048096\n",
      "epoch  800/1000, batch 5, prediction tensor([[-0.1170, -1.7101, -9.6841]], grad_fn=<AddmmBackward0>), loss 0.18512454628944397\n",
      "epoch  800/1000, batch 6, prediction tensor([[-4.3920, -1.2062,  3.0008]], grad_fn=<AddmmBackward0>), loss 0.01538797840476036\n",
      "epoch  800/1000, batch 7, prediction tensor([[-3.6981, -0.7865,  0.9923]], grad_fn=<AddmmBackward0>), loss 0.1638411283493042\n",
      "epoch  820/1000, batch 0, prediction tensor([[-0.4573,  0.2109, -6.9430]], grad_fn=<AddmmBackward0>), loss 0.41436558961868286\n",
      "epoch  820/1000, batch 1, prediction tensor([[-3.8036, -0.7069,  1.0182]], grad_fn=<AddmmBackward0>), loss 0.1707722246646881\n",
      "epoch  820/1000, batch 2, prediction tensor([[-7.3615,  0.9612,  1.9335]], grad_fn=<AddmmBackward0>), loss 0.3208390474319458\n",
      "epoch  820/1000, batch 3, prediction tensor([[ 0.0149, -2.0618, -9.4643]], grad_fn=<AddmmBackward0>), loss 0.11816581338644028\n",
      "epoch  820/1000, batch 4, prediction tensor([[-0.5200, -2.0006, -7.4115]], grad_fn=<AddmmBackward0>), loss 0.2058001309633255\n",
      "epoch  820/1000, batch 5, prediction tensor([[-4.3692, -1.3339,  3.1057]], grad_fn=<AddmmBackward0>), loss 0.01229163445532322\n",
      "epoch  820/1000, batch 6, prediction tensor([[-6.8928,  0.6733,  0.2972]], grad_fn=<AddmmBackward0>), loss 0.5229836106300354\n",
      "epoch  820/1000, batch 7, prediction tensor([[-1.8137, -2.5577, -5.1031]], grad_fn=<AddmmBackward0>), loss 1.1577624082565308\n",
      "epoch  840/1000, batch 0, prediction tensor([[-7.5798,  1.2285,  1.8846]], grad_fn=<AddmmBackward0>), loss 0.41800549626350403\n",
      "epoch  840/1000, batch 1, prediction tensor([[-0.4412, -1.4305, -9.6395]], grad_fn=<AddmmBackward0>), loss 0.3162107765674591\n",
      "epoch  840/1000, batch 2, prediction tensor([[-0.7151, -1.6624, -7.5545]], grad_fn=<AddmmBackward0>), loss 0.3284800052642822\n",
      "epoch  840/1000, batch 3, prediction tensor([[-6.9325,  0.7937,  0.2164]], grad_fn=<AddmmBackward0>), loss 0.44585952162742615\n",
      "epoch  840/1000, batch 4, prediction tensor([[-0.4309,  0.3593, -7.1179]], grad_fn=<AddmmBackward0>), loss 0.37453988194465637\n",
      "epoch  840/1000, batch 5, prediction tensor([[-2.1860, -2.1083, -5.1802]], grad_fn=<AddmmBackward0>), loss 0.6787891983985901\n",
      "epoch  840/1000, batch 6, prediction tensor([[-3.9737, -0.4644,  0.9459]], grad_fn=<AddmmBackward0>), loss 0.224244624376297\n",
      "epoch  840/1000, batch 7, prediction tensor([[-4.6388, -1.0388,  3.0801]], grad_fn=<AddmmBackward0>), loss 0.016568917781114578\n",
      "epoch  860/1000, batch 0, prediction tensor([[-7.0741,  1.0151,  0.1366]], grad_fn=<AddmmBackward0>), loss 0.3476392328739166\n",
      "epoch  860/1000, batch 1, prediction tensor([[-4.5524, -1.1476,  3.1025]], grad_fn=<AddmmBackward0>), loss 0.01462912280112505\n",
      "epoch  860/1000, batch 2, prediction tensor([[  0.1700,  -1.6392, -10.0419]], grad_fn=<AddmmBackward0>), loss 0.15171478688716888\n",
      "epoch  860/1000, batch 3, prediction tensor([[-1.9545, -2.2923, -5.2276]], grad_fn=<AddmmBackward0>), loss 0.8981162309646606\n",
      "epoch  860/1000, batch 4, prediction tensor([[-0.7980,  0.8218, -7.2132]], grad_fn=<AddmmBackward0>), loss 0.1808745414018631\n",
      "epoch  860/1000, batch 5, prediction tensor([[-3.9685, -0.4600,  0.9362]], grad_fn=<AddmmBackward0>), loss 0.22709748148918152\n",
      "epoch  860/1000, batch 6, prediction tensor([[-1.0932, -1.0011, -7.8378]], grad_fn=<AddmmBackward0>), loss 0.7407810688018799\n",
      "epoch  860/1000, batch 7, prediction tensor([[-7.3972,  1.0501,  1.8803]], grad_fn=<AddmmBackward0>), loss 0.36189907789230347\n",
      "epoch  880/1000, batch 0, prediction tensor([[-7.1762,  1.1238,  0.1300]], grad_fn=<AddmmBackward0>), loss 0.3151209056377411\n",
      "epoch  880/1000, batch 1, prediction tensor([[  0.0969,  -1.5050, -10.1031]], grad_fn=<AddmmBackward0>), loss 0.1836092174053192\n",
      "epoch  880/1000, batch 2, prediction tensor([[-7.4310,  1.0874,  1.8768]], grad_fn=<AddmmBackward0>), loss 0.37446045875549316\n",
      "epoch  880/1000, batch 3, prediction tensor([[-2.0215, -2.3543, -5.0987]], grad_fn=<AddmmBackward0>), loss 0.8997892737388611\n",
      "epoch  880/1000, batch 4, prediction tensor([[-0.8133,  0.7431, -7.1192]], grad_fn=<AddmmBackward0>), loss 0.191682368516922\n",
      "epoch  880/1000, batch 5, prediction tensor([[-1.1308, -0.9975, -7.8038]], grad_fn=<AddmmBackward0>), loss 0.7625895738601685\n",
      "epoch  880/1000, batch 6, prediction tensor([[-3.7976, -0.7159,  1.0213]], grad_fn=<AddmmBackward0>), loss 0.16898523271083832\n",
      "epoch  880/1000, batch 7, prediction tensor([[-4.6185, -1.1772,  3.1983]], grad_fn=<AddmmBackward0>), loss 0.012900938279926777\n",
      "epoch  900/1000, batch 0, prediction tensor([[-0.4971,  0.4984, -7.1908]], grad_fn=<AddmmBackward0>), loss 0.3148162364959717\n",
      "epoch  900/1000, batch 1, prediction tensor([[-4.7105, -1.1065,  3.2196]], grad_fn=<AddmmBackward0>), loss 0.013487759977579117\n",
      "epoch  900/1000, batch 2, prediction tensor([[-3.9043, -0.6029,  1.0149]], grad_fn=<AddmmBackward0>), loss 0.18700438737869263\n",
      "epoch  900/1000, batch 3, prediction tensor([[-7.6959,  1.1982,  2.0310]], grad_fn=<AddmmBackward0>), loss 0.3610910475254059\n",
      "epoch  900/1000, batch 4, prediction tensor([[-2.3576, -2.1559, -4.9610]], grad_fn=<AddmmBackward0>), loss 0.6301429271697998\n",
      "epoch  900/1000, batch 5, prediction tensor([[-7.6170,  1.4495,  0.2452]], grad_fn=<AddmmBackward0>), loss 0.26235780119895935\n",
      "epoch  900/1000, batch 6, prediction tensor([[ -0.5653,  -0.9401, -10.0057]], grad_fn=<AddmmBackward0>), loss 0.5232348442077637\n",
      "epoch  900/1000, batch 7, prediction tensor([[-0.6724, -1.4140, -7.8457]], grad_fn=<AddmmBackward0>), loss 0.39010363817214966\n",
      "epoch  920/1000, batch 0, prediction tensor([[-3.7555, -0.8340,  1.0972]], grad_fn=<AddmmBackward0>), loss 0.14217522740364075\n",
      "epoch  920/1000, batch 1, prediction tensor([[-1.9346, -2.5370, -5.0028]], grad_fn=<AddmmBackward0>), loss 1.0686229467391968\n",
      "epoch  920/1000, batch 2, prediction tensor([[-0.9167, -1.2653, -7.7501]], grad_fn=<AddmmBackward0>), loss 0.5345680117607117\n",
      "epoch  920/1000, batch 3, prediction tensor([[-7.1862,  1.0289,  0.2349]], grad_fn=<AddmmBackward0>), loss 0.3731530010700226\n",
      "epoch  920/1000, batch 4, prediction tensor([[-0.4413,  0.5263, -7.2744]], grad_fn=<AddmmBackward0>), loss 0.32236990332603455\n",
      "epoch  920/1000, batch 5, prediction tensor([[-4.7543, -1.0961,  3.2530]], grad_fn=<AddmmBackward0>), loss 0.013163921423256397\n",
      "epoch  920/1000, batch 6, prediction tensor([[  0.0399,  -1.3936, -10.1575]], grad_fn=<AddmmBackward0>), loss 0.21389682590961456\n",
      "epoch  920/1000, batch 7, prediction tensor([[-7.6278,  1.1644,  1.9966]], grad_fn=<AddmmBackward0>), loss 0.36126598715782166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  940/1000, batch 0, prediction tensor([[-7.8011,  1.1542,  2.1801]], grad_fn=<AddmmBackward0>), loss 0.3063901960849762\n",
      "epoch  940/1000, batch 1, prediction tensor([[-4.7847, -1.1902,  3.3775]], grad_fn=<AddmmBackward0>), loss 0.010611065663397312\n",
      "epoch  940/1000, batch 2, prediction tensor([[-7.3873,  1.0975,  0.3675]], grad_fn=<AddmmBackward0>), loss 0.3934837281703949\n",
      "epoch  940/1000, batch 3, prediction tensor([[-2.2768, -2.0508, -5.1468]], grad_fn=<AddmmBackward0>), loss 0.6113722920417786\n",
      "epoch  940/1000, batch 4, prediction tensor([[-0.8951,  0.9353, -7.2295]], grad_fn=<AddmmBackward0>), loss 0.14897096157073975\n",
      "epoch  940/1000, batch 5, prediction tensor([[ -0.5149,  -0.8730, -10.1233]], grad_fn=<AddmmBackward0>), loss 0.530110239982605\n",
      "epoch  940/1000, batch 6, prediction tensor([[-0.6397, -1.3619, -7.9305]], grad_fn=<AddmmBackward0>), loss 0.3963424265384674\n",
      "epoch  940/1000, batch 7, prediction tensor([[-3.7898, -0.7612,  1.0588]], grad_fn=<AddmmBackward0>), loss 0.1568930596113205\n",
      "epoch  960/1000, batch 0, prediction tensor([[-7.8351,  1.1957,  2.1727]], grad_fn=<AddmmBackward0>), loss 0.3195249140262604\n",
      "epoch  960/1000, batch 1, prediction tensor([[ 0.2419, -1.7884, -9.9647]], grad_fn=<AddmmBackward0>), loss 0.12339194118976593\n",
      "epoch  960/1000, batch 2, prediction tensor([[-7.2829,  1.0391,  0.3215]], grad_fn=<AddmmBackward0>), loss 0.3975565731525421\n",
      "epoch  960/1000, batch 3, prediction tensor([[-0.3754, -1.5387, -8.0180]], grad_fn=<AddmmBackward0>), loss 0.2722606360912323\n",
      "epoch  960/1000, batch 4, prediction tensor([[-1.8470, -2.3995, -5.2279]], grad_fn=<AddmmBackward0>), loss 1.0284416675567627\n",
      "epoch  960/1000, batch 5, prediction tensor([[-3.9682, -0.5648,  1.0408]], grad_fn=<AddmmBackward0>), loss 0.18850262463092804\n",
      "epoch  960/1000, batch 6, prediction tensor([[-0.6554,  0.7349, -7.2688]], grad_fn=<AddmmBackward0>), loss 0.22261908650398254\n",
      "epoch  960/1000, batch 7, prediction tensor([[-4.9249, -1.0317,  3.3592]], grad_fn=<AddmmBackward0>), loss 0.012563018128275871\n",
      "epoch  980/1000, batch 0, prediction tensor([[-2.7606, -1.5658, -5.1481]], grad_fn=<AddmmBackward0>), loss 0.28560686111450195\n",
      "epoch  980/1000, batch 1, prediction tensor([[-1.2814, -0.6810, -7.9697]], grad_fn=<AddmmBackward0>), loss 1.0381948947906494\n",
      "epoch  980/1000, batch 2, prediction tensor([[-7.4428,  1.3115,  0.2090]], grad_fn=<AddmmBackward0>), loss 0.2868303954601288\n",
      "epoch  980/1000, batch 3, prediction tensor([[-4.8977, -1.0664,  3.3667]], grad_fn=<AddmmBackward0>), loss 0.012062696740031242\n",
      "epoch  980/1000, batch 4, prediction tensor([[-0.5224,  0.7460, -7.4130]], grad_fn=<AddmmBackward0>), loss 0.24807719886302948\n",
      "epoch  980/1000, batch 5, prediction tensor([[-8.0230,  1.4592,  2.0971]], grad_fn=<AddmmBackward0>), loss 0.42424333095550537\n",
      "epoch  980/1000, batch 6, prediction tensor([[-7.8847e-03, -1.3220e+00, -1.0181e+01]], grad_fn=<AddmmBackward0>), loss 0.2380332499742508\n",
      "epoch  980/1000, batch 7, prediction tensor([[-3.9012, -0.7002,  1.1092]], grad_fn=<AddmmBackward0>), loss 0.15736524760723114\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, xy in enumerate(dataloader):\n",
    "        x_train, y_train = xy\n",
    "        prediction = model(x_train)\n",
    "        loss = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print('epoch {:4d}/{}, batch {}, prediction {}, loss {}'.format(\n",
    "                epoch, epochs, batch_idx, prediction, loss\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
