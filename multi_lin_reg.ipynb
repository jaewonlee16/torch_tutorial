{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])\n",
    "        self.y_data = torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_lin_reg_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.3703, -0.2704,  0.2092]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3228], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#model = nn.Linear(3, 1)\n",
    "model = multi_lin_reg_model()\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0/10000 hypothesis: tensor([29.7749, 23.5064])  cost: 19067.792969\n",
      "epoch   0/10000 hypothesis: tensor([95.5924, 83.0437])  cost: 9741.111328\n",
      "epoch   0/10000 hypothesis: 113.47000885009766  cost: 1484.560181\n",
      "epoch 100/10000 hypothesis: tensor([200.7979, 153.8261])  cost: 13.177094\n",
      "epoch 100/10000 hypothesis: tensor([175.3722, 143.8931])  cost: 12.500123\n",
      "epoch 100/10000 hypothesis: 188.27432250976562  cost: 10.721188\n",
      "epoch 200/10000 hypothesis: tensor([197.5741, 174.8059])  cost: 14.728398\n",
      "epoch 200/10000 hypothesis: tensor([143.4541, 186.9240])  cost: 2.908108\n",
      "epoch 200/10000 hypothesis: 151.6642303466797  cost: 0.112741\n",
      "epoch 300/10000 hypothesis: tensor([144.2333, 153.4271])  cost: 3.512162\n",
      "epoch 300/10000 hypothesis: tensor([187.1827, 176.5177])  cost: 8.445354\n",
      "epoch 300/10000 hypothesis: 199.0968780517578  cost: 9.590653\n",
      "epoch 400/10000 hypothesis: tensor([152.1044, 197.5816])  cost: 1.256181\n",
      "epoch 400/10000 hypothesis: tensor([175.5619, 185.5438])  cost: 9.996250\n",
      "epoch 400/10000 hypothesis: 143.15687561035156  cost: 1.338361\n",
      "epoch 500/10000 hypothesis: tensor([176.1508, 142.5261])  cost: 7.546446\n",
      "epoch 500/10000 hypothesis: tensor([198.0802, 186.4094])  cost: 3.156863\n",
      "epoch 500/10000 hypothesis: 151.92881774902344  cost: 0.005067\n",
      "epoch 600/10000 hypothesis: tensor([186.8930, 177.8612])  cost: 4.078958\n",
      "epoch 600/10000 hypothesis: tensor([198.6730, 143.5688])  cost: 4.802980\n",
      "epoch 600/10000 hypothesis: 152.47108459472656  cost: 0.221921\n",
      "epoch 700/10000 hypothesis: tensor([142.5329, 177.0164])  cost: 4.592886\n",
      "epoch 700/10000 hypothesis: tensor([197.8629, 186.1396])  cost: 2.384521\n",
      "epoch 700/10000 hypothesis: 152.19381713867188  cost: 0.037565\n",
      "epoch 800/10000 hypothesis: tensor([177.3237, 142.5017])  cost: 3.707165\n",
      "epoch 800/10000 hypothesis: tensor([197.7323, 152.8773])  cost: 1.885276\n",
      "epoch 800/10000 hypothesis: 185.34939575195312  cost: 0.122077\n",
      "epoch 900/10000 hypothesis: tensor([177.0181, 142.0102])  cost: 4.445880\n",
      "epoch 900/10000 hypothesis: tensor([197.1997, 185.4859])  cost: 0.837643\n",
      "epoch 900/10000 hypothesis: 152.21038818359375  cost: 0.044263\n",
      "epoch1000/10000 hypothesis: tensor([185.0310, 152.3074])  cost: 0.047739\n",
      "epoch1000/10000 hypothesis: tensor([196.6430, 142.1725])  cost: 0.221609\n",
      "epoch1000/10000 hypothesis: 177.29486083984375  cost: 7.317778\n",
      "epoch1100/10000 hypothesis: tensor([196.6516, 142.1975])  cost: 0.231774\n",
      "epoch1100/10000 hypothesis: tensor([152.1677, 177.5620])  cost: 2.985917\n",
      "epoch1100/10000 hypothesis: 185.29177856445312  cost: 0.085135\n",
      "epoch1200/10000 hypothesis: tensor([152.3861, 177.9591])  cost: 2.157083\n",
      "epoch1200/10000 hypothesis: tensor([142.4931, 197.0379])  cost: 0.660181\n",
      "epoch1200/10000 hypothesis: 184.9456329345703  cost: 0.002956\n",
      "epoch1300/10000 hypothesis: tensor([152.2895, 177.9905])  cost: 2.060863\n",
      "epoch1300/10000 hypothesis: tensor([185.1355, 142.3766])  cost: 0.080102\n",
      "epoch1300/10000 hypothesis: 196.7252197265625  cost: 0.525944\n",
      "epoch1400/10000 hypothesis: tensor([184.8965, 142.2152])  cost: 0.028524\n",
      "epoch1400/10000 hypothesis: tensor([178.3331, 152.4800])  cost: 1.504444\n",
      "epoch1400/10000 hypothesis: 196.87896728515625  cost: 0.772583\n",
      "epoch1500/10000 hypothesis: tensor([185.4620, 179.0589])  cost: 0.549603\n",
      "epoch1500/10000 hypothesis: tensor([197.2944, 142.7495])  cost: 1.118589\n",
      "epoch1500/10000 hypothesis: 152.69361877441406  cost: 0.481107\n",
      "epoch1600/10000 hypothesis: tensor([142.2290, 152.5502])  cost: 0.177618\n",
      "epoch1600/10000 hypothesis: tensor([184.7029, 178.4922])  cost: 1.180825\n",
      "epoch1600/10000 hypothesis: 196.8190155029297  cost: 0.670786\n",
      "epoch1700/10000 hypothesis: tensor([178.8096, 152.6043])  cost: 0.891129\n",
      "epoch1700/10000 hypothesis: tensor([196.7063, 185.0376])  cost: 0.250147\n",
      "epoch1700/10000 hypothesis: 142.2311248779297  cost: 0.053419\n",
      "epoch1800/10000 hypothesis: tensor([178.7498, 142.1362])  cost: 0.790758\n",
      "epoch1800/10000 hypothesis: tensor([184.9607, 196.6120])  cost: 0.188033\n",
      "epoch1800/10000 hypothesis: 152.5667724609375  cost: 0.321231\n",
      "epoch1900/10000 hypothesis: tensor([152.9796, 179.4198])  cost: 0.648076\n",
      "epoch1900/10000 hypothesis: tensor([185.2123, 196.8689])  cost: 0.400028\n",
      "epoch1900/10000 hypothesis: 142.33401489257812  cost: 0.111566\n",
      "epoch2000/10000 hypothesis: tensor([196.5034, 184.8839])  cost: 0.133451\n",
      "epoch2000/10000 hypothesis: tensor([142.2378, 179.0432])  cost: 0.486048\n",
      "epoch2000/10000 hypothesis: 152.72970581054688  cost: 0.532471\n",
      "epoch2100/10000 hypothesis: tensor([179.0549, 142.1894])  cost: 0.464506\n",
      "epoch2100/10000 hypothesis: tensor([196.4869, 184.8810])  cost: 0.125632\n",
      "epoch2100/10000 hypothesis: 152.59353637695312  cost: 0.352285\n",
      "epoch2200/10000 hypothesis: tensor([179.2460, 184.8088])  cost: 0.302525\n",
      "epoch2200/10000 hypothesis: tensor([152.8153, 142.4660])  cost: 0.440989\n",
      "epoch2200/10000 hypothesis: 196.36134338378906  cost: 0.130569\n",
      "epoch2300/10000 hypothesis: tensor([152.4975, 142.1843])  cost: 0.140712\n",
      "epoch2300/10000 hypothesis: tensor([196.0604, 184.5093])  cost: 0.122238\n",
      "epoch2300/10000 hypothesis: 179.13023376464844  cost: 0.756493\n",
      "epoch2400/10000 hypothesis: tensor([196.0599, 179.1007])  cost: 0.406141\n",
      "epoch2400/10000 hypothesis: tensor([184.7214, 152.5569])  cost: 0.193881\n",
      "epoch2400/10000 hypothesis: 142.21971130371094  cost: 0.048273\n",
      "epoch2500/10000 hypothesis: tensor([142.1723, 184.5961])  cost: 0.096416\n",
      "epoch2500/10000 hypothesis: tensor([196.1960, 179.2902])  cost: 0.271083\n",
      "epoch2500/10000 hypothesis: 152.60629272460938  cost: 0.367591\n",
      "epoch2600/10000 hypothesis: tensor([152.6381, 196.3494])  cost: 0.264655\n",
      "epoch2600/10000 hypothesis: tensor([179.2748, 142.1845])  cost: 0.279999\n",
      "epoch2600/10000 hypothesis: 184.7317352294922  cost: 0.071966\n",
      "epoch2700/10000 hypothesis: tensor([142.3691, 196.3351])  cost: 0.124246\n",
      "epoch2700/10000 hypothesis: tensor([184.6612, 152.5001])  cost: 0.182438\n",
      "epoch2700/10000 hypothesis: 179.36029052734375  cost: 0.409228\n",
      "epoch2800/10000 hypothesis: tensor([152.6626, 184.8623])  cost: 0.228974\n",
      "epoch2800/10000 hypothesis: tensor([142.3323, 196.2536])  cost: 0.087384\n",
      "epoch2800/10000 hypothesis: 179.384521484375  cost: 0.378814\n",
      "epoch2900/10000 hypothesis: tensor([179.4750, 196.1555])  cost: 0.149905\n",
      "epoch2900/10000 hypothesis: tensor([152.5767, 142.3456])  cost: 0.226036\n",
      "epoch2900/10000 hypothesis: 184.57608032226562  cost: 0.179708\n",
      "epoch3000/10000 hypothesis: tensor([152.4455, 184.6094])  cost: 0.175540\n",
      "epoch3000/10000 hypothesis: tensor([142.2432, 196.0732])  cost: 0.032258\n",
      "epoch3000/10000 hypothesis: 179.38638305664062  cost: 0.376526\n",
      "epoch3100/10000 hypothesis: tensor([195.9933, 179.4208])  cost: 0.167746\n",
      "epoch3100/10000 hypothesis: tensor([184.6910, 152.5080])  cost: 0.176760\n",
      "epoch3100/10000 hypothesis: 142.28977966308594  cost: 0.083972\n",
      "epoch3200/10000 hypothesis: tensor([152.6400, 179.7393])  cost: 0.238814\n",
      "epoch3200/10000 hypothesis: tensor([142.3904, 184.7829])  cost: 0.099775\n",
      "epoch3200/10000 hypothesis: 196.2083740234375  cost: 0.043420\n",
      "epoch3300/10000 hypothesis: tensor([179.7567, 196.2891])  cost: 0.071369\n",
      "epoch3300/10000 hypothesis: tensor([142.4342, 184.8292])  cost: 0.108859\n",
      "epoch3300/10000 hypothesis: 152.57826232910156  cost: 0.334387\n",
      "epoch3400/10000 hypothesis: tensor([152.4508, 184.6457])  cost: 0.164375\n",
      "epoch3400/10000 hypothesis: tensor([179.5798, 142.2989])  cost: 0.132965\n",
      "epoch3400/10000 hypothesis: 196.0989227294922  cost: 0.009786\n",
      "epoch3500/10000 hypothesis: tensor([196.2635, 142.4633])  cost: 0.142060\n",
      "epoch3500/10000 hypothesis: tensor([179.6460, 152.4795])  cost: 0.177623\n",
      "epoch3500/10000 hypothesis: 184.67123413085938  cost: 0.108087\n",
      "epoch3600/10000 hypothesis: tensor([152.5737, 142.4427])  cost: 0.262521\n",
      "epoch3600/10000 hypothesis: tensor([179.5860, 195.9944])  cost: 0.085721\n",
      "epoch3600/10000 hypothesis: 184.70530700683594  cost: 0.086844\n",
      "epoch3700/10000 hypothesis: tensor([179.7943, 184.8053])  cost: 0.040106\n",
      "epoch3700/10000 hypothesis: tensor([152.6456, 142.5208])  cost: 0.344028\n",
      "epoch3700/10000 hypothesis: 196.05294799804688  cost: 0.002803\n",
      "epoch3800/10000 hypothesis: tensor([152.4193, 179.6510])  cost: 0.148810\n",
      "epoch3800/10000 hypothesis: tensor([184.6347, 196.0067])  cost: 0.066759\n",
      "epoch3800/10000 hypothesis: 142.39096069335938  cost: 0.152850\n",
      "epoch3900/10000 hypothesis: tensor([152.4082, 179.6599])  cost: 0.141147\n",
      "epoch3900/10000 hypothesis: tensor([142.3257, 195.9928])  cost: 0.053076\n",
      "epoch3900/10000 hypothesis: 184.57032775878906  cost: 0.184618\n",
      "epoch4000/10000 hypothesis: tensor([184.6975, 142.3823])  cost: 0.118822\n",
      "epoch4000/10000 hypothesis: tensor([179.7364, 196.0595])  cost: 0.036509\n",
      "epoch4000/10000 hypothesis: 152.49627685546875  cost: 0.246291\n",
      "epoch4100/10000 hypothesis: tensor([179.5614, 152.2879])  cost: 0.137626\n",
      "epoch4100/10000 hypothesis: tensor([142.2742, 195.8887])  cost: 0.043796\n",
      "epoch4100/10000 hypothesis: 184.52621459960938  cost: 0.224473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4200/10000 hypothesis: tensor([184.7686, 179.8249])  cost: 0.042103\n",
      "epoch4200/10000 hypothesis: tensor([142.5231, 196.2236])  cost: 0.161808\n",
      "epoch4200/10000 hypothesis: 152.4521484375  cost: 0.204438\n",
      "epoch4300/10000 hypothesis: tensor([142.2794, 195.8672])  cost: 0.047838\n",
      "epoch4300/10000 hypothesis: tensor([152.2937, 184.5248])  cost: 0.156021\n",
      "epoch4300/10000 hypothesis: 179.6573028564453  cost: 0.117441\n",
      "epoch4400/10000 hypothesis: tensor([184.7826, 142.4674])  cost: 0.132853\n",
      "epoch4400/10000 hypothesis: tensor([152.4711, 179.8204])  cost: 0.127095\n",
      "epoch4400/10000 hypothesis: 196.01971435546875  cost: 0.000389\n",
      "epoch4500/10000 hypothesis: tensor([142.4911, 179.8860])  cost: 0.127078\n",
      "epoch4500/10000 hypothesis: tensor([184.7415, 152.4608])  cost: 0.139583\n",
      "epoch4500/10000 hypothesis: 196.03265380859375  cost: 0.001066\n",
      "epoch4600/10000 hypothesis: tensor([179.8862, 184.8001])  cost: 0.026455\n",
      "epoch4600/10000 hypothesis: tensor([196.2058, 142.5475])  cost: 0.171039\n",
      "epoch4600/10000 hypothesis: 152.43601989746094  cost: 0.190113\n",
      "epoch4700/10000 hypothesis: tensor([152.4637, 184.7546])  cost: 0.137628\n",
      "epoch4700/10000 hypothesis: tensor([196.0318, 179.8179])  cost: 0.017090\n",
      "epoch4700/10000 hypothesis: 142.45701599121094  cost: 0.208864\n",
      "epoch4800/10000 hypothesis: tensor([184.6443, 142.3767])  cost: 0.134226\n",
      "epoch4800/10000 hypothesis: tensor([179.7658, 195.9632])  cost: 0.028098\n",
      "epoch4800/10000 hypothesis: 152.4337615966797  cost: 0.188149\n",
      "epoch4900/10000 hypothesis: tensor([142.4953, 179.9008])  cost: 0.127566\n",
      "epoch4900/10000 hypothesis: tensor([196.0265, 184.7245])  cost: 0.038296\n",
      "epoch4900/10000 hypothesis: 152.4801788330078  cost: 0.230572\n",
      "epoch5000/10000 hypothesis: tensor([142.2734, 195.7818])  cost: 0.061187\n",
      "epoch5000/10000 hypothesis: tensor([179.6354, 152.2450])  cost: 0.096479\n",
      "epoch5000/10000 hypothesis: 184.5425567626953  cost: 0.209254\n",
      "epoch5100/10000 hypothesis: tensor([142.3446, 195.8723])  cost: 0.067529\n",
      "epoch5100/10000 hypothesis: tensor([152.2824, 184.5584])  cost: 0.137381\n",
      "epoch5100/10000 hypothesis: 179.7366943359375  cost: 0.069330\n",
      "epoch5200/10000 hypothesis: tensor([142.4804, 184.7642])  cost: 0.143166\n",
      "epoch5200/10000 hypothesis: tensor([179.8569, 152.4224])  cost: 0.099457\n",
      "epoch5200/10000 hypothesis: 195.96173095703125  cost: 0.001465\n",
      "epoch5300/10000 hypothesis: tensor([179.7132, 142.3382])  cost: 0.098308\n",
      "epoch5300/10000 hypothesis: tensor([184.5782, 152.2923])  cost: 0.131661\n",
      "epoch5300/10000 hypothesis: 195.89971923828125  cost: 0.010056\n",
      "epoch5400/10000 hypothesis: tensor([142.3098, 184.5333])  cost: 0.156873\n",
      "epoch5400/10000 hypothesis: tensor([179.7296, 152.2955])  cost: 0.080236\n",
      "epoch5400/10000 hypothesis: 195.8621063232422  cost: 0.019015\n",
      "epoch5500/10000 hypothesis: tensor([184.5520, 195.8130])  cost: 0.117842\n",
      "epoch5500/10000 hypothesis: tensor([152.3930, 142.4503])  cost: 0.178632\n",
      "epoch5500/10000 hypothesis: 179.6885223388672  cost: 0.097018\n",
      "epoch5600/10000 hypothesis: tensor([142.3308, 184.5553])  cost: 0.153574\n",
      "epoch5600/10000 hypothesis: tensor([152.2991, 179.7474])  cost: 0.076658\n",
      "epoch5600/10000 hypothesis: 195.86314392089844  cost: 0.018730\n",
      "epoch5700/10000 hypothesis: tensor([179.7033, 184.5532])  cost: 0.143820\n",
      "epoch5700/10000 hypothesis: tensor([196.0045, 152.4067])  cost: 0.082722\n",
      "epoch5700/10000 hypothesis: 142.40457153320312  cost: 0.163678\n",
      "epoch5800/10000 hypothesis: tensor([142.4448, 195.9643])  cost: 0.099559\n",
      "epoch5800/10000 hypothesis: tensor([152.3127, 184.6250])  cost: 0.119229\n",
      "epoch5800/10000 hypothesis: 179.80169677734375  cost: 0.039324\n",
      "epoch5900/10000 hypothesis: tensor([152.3890, 142.4606])  cost: 0.181742\n",
      "epoch5900/10000 hypothesis: tensor([195.8006, 184.5501])  cost: 0.121079\n",
      "epoch5900/10000 hypothesis: 179.86451721191406  cost: 0.018356\n",
      "epoch6000/10000 hypothesis: tensor([195.7703, 152.2247])  cost: 0.051631\n",
      "epoch6000/10000 hypothesis: tensor([142.3260, 179.7013])  cost: 0.097753\n",
      "epoch6000/10000 hypothesis: 184.54837036132812  cost: 0.203969\n",
      "epoch6100/10000 hypothesis: tensor([152.3858, 195.9813])  cost: 0.074580\n",
      "epoch6100/10000 hypothesis: tensor([184.6505, 142.4127])  cost: 0.146227\n",
      "epoch6100/10000 hypothesis: 179.81300354003906  cost: 0.034968\n",
      "epoch6200/10000 hypothesis: tensor([196.0340, 184.7759])  cost: 0.025678\n",
      "epoch6200/10000 hypothesis: tensor([152.4661, 179.9717])  cost: 0.109029\n",
      "epoch6200/10000 hypothesis: 142.476318359375  cost: 0.226879\n",
      "epoch6300/10000 hypothesis: tensor([179.9790, 196.0887])  cost: 0.004151\n",
      "epoch6300/10000 hypothesis: tensor([152.4538, 142.5357])  cost: 0.246493\n",
      "epoch6300/10000 hypothesis: 184.61264038085938  cost: 0.150047\n",
      "epoch6400/10000 hypothesis: tensor([152.4754, 184.8405])  cost: 0.125705\n",
      "epoch6400/10000 hypothesis: tensor([179.9348, 142.5164])  cost: 0.135439\n",
      "epoch6400/10000 hypothesis: 195.9456787109375  cost: 0.002951\n",
      "epoch6500/10000 hypothesis: tensor([142.5387, 184.8093])  cost: 0.163262\n",
      "epoch6500/10000 hypothesis: tensor([179.9103, 152.4015])  cost: 0.084614\n",
      "epoch6500/10000 hypothesis: 195.93667602539062  cost: 0.004010\n",
      "epoch6600/10000 hypothesis: tensor([142.4468, 152.3436])  cost: 0.158857\n",
      "epoch6600/10000 hypothesis: tensor([179.6994, 184.5304])  cost: 0.155409\n",
      "epoch6600/10000 hypothesis: 195.96405029296875  cost: 0.001292\n",
      "epoch6700/10000 hypothesis: tensor([142.3303, 195.7614])  cost: 0.083011\n",
      "epoch6700/10000 hypothesis: tensor([152.2135, 179.7043])  cost: 0.066518\n",
      "epoch6700/10000 hypothesis: 184.55947875976562  cost: 0.194059\n",
      "epoch6800/10000 hypothesis: tensor([184.5770, 142.3659])  cost: 0.156387\n",
      "epoch6800/10000 hypothesis: tensor([179.7780, 195.8438])  cost: 0.036836\n",
      "epoch6800/10000 hypothesis: 152.35304260253906  cost: 0.124639\n",
      "epoch6900/10000 hypothesis: tensor([152.2550, 184.5899])  cost: 0.116585\n",
      "epoch6900/10000 hypothesis: tensor([195.8713, 179.8063])  cost: 0.027053\n",
      "epoch6900/10000 hypothesis: 142.4757843017578  cost: 0.226371\n",
      "epoch7000/10000 hypothesis: tensor([184.7317, 152.3713])  cost: 0.104918\n",
      "epoch7000/10000 hypothesis: tensor([142.4779, 195.9577])  cost: 0.115097\n",
      "epoch7000/10000 hypothesis: 179.80963134765625  cost: 0.036240\n",
      "epoch7100/10000 hypothesis: tensor([152.3763, 184.7405])  cost: 0.104464\n",
      "epoch7100/10000 hypothesis: tensor([195.9617, 179.8910])  cost: 0.006673\n",
      "epoch7100/10000 hypothesis: 142.51072692871094  cost: 0.260842\n",
      "epoch7200/10000 hypothesis: tensor([195.9693, 179.8994])  cost: 0.005533\n",
      "epoch7200/10000 hypothesis: tensor([142.5155, 184.7683])  cost: 0.159734\n",
      "epoch7200/10000 hypothesis: 152.36302185058594  cost: 0.131785\n",
      "epoch7300/10000 hypothesis: tensor([179.8737, 184.7093])  cost: 0.050236\n",
      "epoch7300/10000 hypothesis: tensor([142.5501, 196.0483])  cost: 0.152479\n",
      "epoch7300/10000 hypothesis: 152.3319091796875  cost: 0.110164\n",
      "epoch7400/10000 hypothesis: tensor([184.5429, 142.3459])  cost: 0.164298\n",
      "epoch7400/10000 hypothesis: tensor([195.8092, 179.7616])  cost: 0.046600\n",
      "epoch7400/10000 hypothesis: 152.33328247070312  cost: 0.111077\n",
      "epoch7500/10000 hypothesis: tensor([142.4807, 195.9458])  cost: 0.116989\n",
      "epoch7500/10000 hypothesis: tensor([184.6419, 179.8095])  cost: 0.082259\n",
      "epoch7500/10000 hypothesis: 152.39700317382812  cost: 0.157612\n",
      "epoch7600/10000 hypothesis: tensor([195.8225, 179.7761])  cost: 0.040808\n",
      "epoch7600/10000 hypothesis: tensor([184.7077, 152.3359])  cost: 0.099141\n",
      "epoch7600/10000 hypothesis: 142.47593688964844  cost: 0.226516\n",
      "epoch7700/10000 hypothesis: tensor([179.8921, 195.9525])  cost: 0.006946\n",
      "epoch7700/10000 hypothesis: tensor([184.7670, 152.3846])  cost: 0.101116\n",
      "epoch7700/10000 hypothesis: 142.50244140625  cost: 0.252447\n",
      "epoch7800/10000 hypothesis: tensor([179.9130, 195.9734])  cost: 0.004141\n",
      "epoch7800/10000 hypothesis: tensor([142.5278, 152.3915])  cost: 0.215935\n",
      "epoch7800/10000 hypothesis: 184.59512329101562  cost: 0.163925\n",
      "epoch7900/10000 hypothesis: tensor([184.5611, 179.7330])  cost: 0.131950\n",
      "epoch7900/10000 hypothesis: tensor([142.4957, 152.3534])  cost: 0.185279\n",
      "epoch7900/10000 hypothesis: 195.77500915527344  cost: 0.050621\n",
      "epoch8000/10000 hypothesis: tensor([184.4963, 142.3145])  cost: 0.176300\n",
      "epoch8000/10000 hypothesis: tensor([179.7356, 152.2098])  cost: 0.056962\n",
      "epoch8000/10000 hypothesis: 195.79063415527344  cost: 0.043834\n",
      "epoch8100/10000 hypothesis: tensor([152.2018, 179.7282])  cost: 0.057310\n",
      "epoch8100/10000 hypothesis: tensor([142.3762, 195.7844])  cost: 0.093996\n",
      "epoch8100/10000 hypothesis: 184.5625  cost: 0.191406\n",
      "epoch8200/10000 hypothesis: tensor([184.8124, 152.4162])  cost: 0.104196\n",
      "epoch8200/10000 hypothesis: tensor([179.9356, 142.5261])  cost: 0.140487\n",
      "epoch8200/10000 hypothesis: 195.903076171875  cost: 0.009394\n",
      "epoch8300/10000 hypothesis: tensor([195.7824, 184.5756])  cost: 0.113757\n",
      "epoch8300/10000 hypothesis: tensor([179.9069, 142.5018])  cost: 0.130243\n",
      "epoch8300/10000 hypothesis: 152.2947998046875  cost: 0.086907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8400/10000 hypothesis: tensor([152.1777, 195.7295])  cost: 0.052366\n",
      "epoch8400/10000 hypothesis: tensor([142.3666, 184.5627])  cost: 0.162837\n",
      "epoch8400/10000 hypothesis: 179.77578735351562  cost: 0.050271\n",
      "epoch8500/10000 hypothesis: tensor([195.7723, 152.2115])  cost: 0.048275\n",
      "epoch8500/10000 hypothesis: tensor([184.5838, 179.7602])  cost: 0.115345\n",
      "epoch8500/10000 hypothesis: 142.5057373046875  cost: 0.255770\n",
      "epoch8600/10000 hypothesis: tensor([142.4984, 179.9062])  cost: 0.128584\n",
      "epoch8600/10000 hypothesis: tensor([152.2926, 184.6625])  cost: 0.099751\n",
      "epoch8600/10000 hypothesis: 195.9009552001953  cost: 0.009810\n",
      "epoch8700/10000 hypothesis: tensor([152.3932, 196.0061])  cost: 0.077341\n",
      "epoch8700/10000 hypothesis: tensor([179.8728, 142.4722])  cost: 0.119564\n",
      "epoch8700/10000 hypothesis: 184.6409454345703  cost: 0.128920\n",
      "epoch8800/10000 hypothesis: tensor([142.3945, 152.2362])  cost: 0.105704\n",
      "epoch8800/10000 hypothesis: tensor([195.6714, 184.4738])  cost: 0.192418\n",
      "epoch8800/10000 hypothesis: 179.86497497558594  cost: 0.018232\n",
      "epoch8900/10000 hypothesis: tensor([152.3701, 179.9285])  cost: 0.071028\n",
      "epoch8900/10000 hypothesis: tensor([195.9130, 184.6995])  cost: 0.048948\n",
      "epoch8900/10000 hypothesis: 142.5471954345703  cost: 0.299423\n",
      "epoch9000/10000 hypothesis: tensor([195.7476, 179.7253])  cost: 0.069583\n",
      "epoch9000/10000 hypothesis: tensor([152.3014, 142.4574])  cost: 0.150034\n",
      "epoch9000/10000 hypothesis: 184.52951049804688  cost: 0.221360\n",
      "epoch9100/10000 hypothesis: tensor([195.9500, 179.9062])  cost: 0.005650\n",
      "epoch9100/10000 hypothesis: tensor([184.7708, 152.3781])  cost: 0.097734\n",
      "epoch9100/10000 hypothesis: 142.51084899902344  cost: 0.260967\n",
      "epoch9200/10000 hypothesis: tensor([179.9013, 152.3438])  cost: 0.063971\n",
      "epoch9200/10000 hypothesis: tensor([142.4597, 184.6824])  cost: 0.156108\n",
      "epoch9200/10000 hypothesis: 195.88409423828125  cost: 0.013434\n",
      "epoch9300/10000 hypothesis: tensor([179.9048, 184.7326])  cost: 0.040282\n",
      "epoch9300/10000 hypothesis: tensor([196.0433, 142.5663])  cost: 0.161276\n",
      "epoch9300/10000 hypothesis: 152.32135009765625  cost: 0.103266\n",
      "epoch9400/10000 hypothesis: tensor([142.3822, 152.2197])  cost: 0.097190\n",
      "epoch9400/10000 hypothesis: tensor([195.6566, 184.4620])  cost: 0.203707\n",
      "epoch9400/10000 hypothesis: 179.8636016845703  cost: 0.018605\n",
      "epoch9500/10000 hypothesis: tensor([142.3842, 184.5828])  cost: 0.160835\n",
      "epoch9500/10000 hypothesis: tensor([195.8191, 179.7942])  cost: 0.037552\n",
      "epoch9500/10000 hypothesis: 152.32833862304688  cost: 0.107806\n",
      "epoch9600/10000 hypothesis: tensor([184.7352, 179.9082])  cost: 0.039264\n",
      "epoch9600/10000 hypothesis: tensor([152.4198, 196.0428])  cost: 0.089043\n",
      "epoch9600/10000 hypothesis: 142.49267578125  cost: 0.242729\n",
      "epoch9700/10000 hypothesis: tensor([152.2117, 142.3771])  cost: 0.093497\n",
      "epoch9700/10000 hypothesis: tensor([179.6439, 184.4571])  cost: 0.210768\n",
      "epoch9700/10000 hypothesis: 195.886474609375  cost: 0.012888\n",
      "epoch9800/10000 hypothesis: tensor([195.8073, 184.6032])  cost: 0.097285\n",
      "epoch9800/10000 hypothesis: tensor([152.3631, 179.9260])  cost: 0.068643\n",
      "epoch9800/10000 hypothesis: 142.47071838378906  cost: 0.221576\n",
      "epoch9900/10000 hypothesis: tensor([195.7916, 184.5891])  cost: 0.106159\n",
      "epoch9900/10000 hypothesis: tensor([152.3571, 142.5104])  cost: 0.194019\n",
      "epoch9900/10000 hypothesis: 179.75741577148438  cost: 0.058847\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        hypothesis = x_train @ W + b\n",
    "        prediction = model.forward(x_train)\n",
    "\n",
    "        #loss = torch.mean((hypothesis-y_train)**2)\n",
    "    \n",
    "    \n",
    "        loss = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch%100 == 0:\n",
    "            print('epoch{:4d}/{} hypothesis: {}  cost: {:.6f}'.format(\n",
    "                epoch, epochs, prediction.squeeze().detach(), loss.item()\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True) tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train@W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.2289, 0.6517, 0.1428]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3302], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[152.2206],\n",
       "        [184.5819],\n",
       "        [179.7651],\n",
       "        [195.7847],\n",
       "        [142.3835]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train0 @ list(model.parameters())[0].view(3, 1) + list(model.parameters())[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
